# Teste gerenciamento de prompt ide - cosmic garden

**Criado**: 2025-12-07  
**Status**: Cole√ß√£o de Prompts Operacionais

---

## üìã √çndice de Prompts

1. [Prompt #1: Estrutura Topol√≥gica para Roteamento de Agentes](#prompt-1)
2. [Prompt #2: Mapeador de Inten√ß√£o e √çndice Invertido](#prompt-2)
3. [Prompt #3: Prot√≥tipo do Orquestrador - Funil de Contexto](#prompt-3)
4. [Prompt #4: Montador de Prompt - Offloading Cognitivo](#prompt-4)
5. [Prompt #5: Runtime Principal - Ciclo Completo](#prompt-5)

---

<a name="prompt-1"></a>
## Prompt #1: Estrutura Topol√≥gica para Roteamento de Agentes

**Data**: 2025-12-07 06:32  
**Contexto**: Sistema de documenta√ß√£o com arquitetura multiagente

---

Essa √© uma abordagem extremamente sofisticada e promissora, que une Engenharia de Prompt, Arquitetura de Software e Sistemas Multiagente. A ideia de usar a Topologia do sistema de arquivos para impor gravidade e contexto aos agentes √© um excelente contraponto √† "antigravidade" do contexto saturado.

Vamos estruturar o plano de implementa√ß√£o em formato readme_teste.md.

### üìÑ readme_teste.md: Estrutura Topol√≥gica para Roteamento de Agentes

O objetivo deste projeto √© estabelecer uma **Topologia de Documenta√ß√£o Robusta** (baseada em uma √Årvore Bin√°ria ou Estrutura de Pastas Multin√≠vel) que n√£o apenas sirva como refer√™ncia, mas tamb√©m atue como um **Sistema de Roteamento Inteligente para Agentes de IA**.

A posi√ß√£o do arquivo na estrutura de pastas (o "caminho topol√≥gico") ser√° usada pelo **Agente Orquestrador** para buscar prompts de l√≥gica estruturada e snippets de informa√ß√£o relevantes, maximizando a efici√™ncia e o aproveitamento do contexto.

---

### I. üå≥ Fase 1: Estrutura√ß√£o e Algoritmos de Indexa√ß√£o

O primeiro passo √© mapear a estrutura f√≠sica das pastas em um √≠ndice l√≥gico que reflita sua profundidade e relacionamento.

#### 1. Algoritmo de Travessia de √Årvore

O m√©todo mais eficaz para indexar a documenta√ß√£o hier√°rquica e gerar o √çndice (Table of Contents - ToC) √© o **Depth-First Search (DFS)** (Busca em Profundidade).

**Finalidade**: O DFS garante que todos os n√≥s (pastas/arquivos) em um determinado ramo sejam visitados completamente antes de passar para o pr√≥ximo ramo, refletindo a ordem l√≥gica que um leitor seguiria.

**Sa√≠da**: Gera√ß√£o de um arquivo `INDEX.json` ou `TOC.md` que lista todos os documentos com sua profundidade e caminho topol√≥gico.

#### 2. Visualiza√ß√£o Topol√≥gica da Documenta√ß√£o

O documento principal (`index.md`) deve refletir a profundidade do sistema de arquivos atrav√©s de indenta√ß√£o e t√≠tulos multin√≠veis.

| N√≠vel (Profundidade) | Elemento de Documenta√ß√£o | Exemplo de Sa√≠da |
|:---:|:---:|:---|
| 0 | Raiz / T√≠tulo Principal | `# Documenta√ß√£o Mestra` |
| 1 | Subdiret√≥rio / M√≥dulo Principal | `## 1. M√≥dulo de Autentica√ß√£o` |
| 2 | Subpasta / Componente | `### 1.1. L√≥gica JWT` |
| 3 | Arquivo / Detalhe T√©cnico | `* Detalhe sobre Expiration Time` |

---

### II. ü§ñ Fase 2: Arquitetura de Programa√ß√£o Multiagente Topol√≥gica

O agente de IA n√£o deve ter liberdade para abstrair. Sua √∫nica tarefa inicial √© **rotear o prompt para o lugar certo na documenta√ß√£o**, onde reside a l√≥gica estruturada (o prompt pr√©-definido para aquele contexto espec√≠fico).

#### 1. O Agente Orquestrador (Router Principal)

Este agente √© o ponto de entrada e a "gravidade" do sistema.

**Entrada**: `(Query do Usu√°rio + Contexto IDE/Caminho do Arquivo Atual)`

**Fun√ß√£o**: O Orquestrador executa uma **Busca de Similaridade Aumentada por Caminho** (Path-Augmented Retrieval).

- **Vetoriza√ß√£o**: A Query do Usu√°rio √© convertida em um Embedding Vetorial.

- **Busca RAG**: O Embedding busca por similaridade sem√¢ntica em um banco de dados vetorial que cont√©m todos os snippets de l√≥gica agentica (os prompts t√©cnicos).

- **Filtragem Topol√≥gica**: O Orquestrador filtra os resultados mais relevantes, priorizando aqueles cujos metadados de caminho topol√≥gico (e.g., `/auth/jwt/`) s√£o mais pr√≥ximos do caminho do arquivo atual do usu√°rio (e.g., o usu√°rio est√° editando `/src/auth/jwt_service.py`).

#### 2. Os Subagentes de Execu√ß√£o (Prompts T√©cnicos)

Os arquivos nas pastas mais profundas n√£o devem ser documenta√ß√£o livre, mas sim **prompts imediatos** (l√≥gica estruturada).

**Conte√∫do do Arquivo** (Exemplo: `/logic/jwt/validate_token.json`):

```json
{
  "role": "Agent de Valida√ß√£o JWT",
  "instruction_type": "Schema de Sa√≠da Obrigat√≥rio",
  "schema_enforcement": {
    "status": "string (VALID/EXPIRED/INVALID)",
    "user_id": "integer (If VALID)",
    "error_code": "string (If EXPIRED/INVALID)"
  },
  "task": "Dada a STRING_TOKEN fornecida pelo usu√°rio, analise o tempo de expira√ß√£o e a assinatura. Preencha o 'schema_enforcement' estritamente com o resultado da an√°lise."
}
```

**Execu√ß√£o**: O Orquestrador entrega a `Query do Usu√°rio` + o `Prompt T√©cnico Estruturado` acima para o **Agente de Execu√ß√£o**. Este agente n√£o precisa abstrair sobre o formato; ele apenas preenche o schema r√≠gido.

---

### III. ‚öôÔ∏è Fase 3: Estrutura de Pastas Exemplo

A pasta de implementa√ß√£o deve espelhar a l√≥gica.

```
/projeto_agentes_topologicos
‚îú‚îÄ‚îÄ /documentacao_mestra
‚îÇ   ‚îú‚îÄ‚îÄ index.md        (Gerado via DFS, refletindo a estrutura abaixo)
‚îÇ   ‚îî‚îÄ‚îÄ /agentes
‚îÇ       ‚îú‚îÄ‚îÄ /auth
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ /jwt
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validate_token.json   (Prompt de Subagente)
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ refresh_flow.json
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ /oauth2
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ scope_check.json
‚îÇ       ‚îî‚îÄ‚îÄ /database
‚îÇ           ‚îú‚îÄ‚îÄ /queries
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ sql_injection_check.json
‚îÇ           ‚îî‚îÄ‚îÄ /migrations
‚îÇ               ‚îî‚îÄ‚îÄ rollback_logic.json
‚îî‚îÄ‚îÄ /implementacao_agentes
    ‚îî‚îÄ‚îÄ orquestrador.py (L√≥gica do Roteador Principal)
```

**Conclus√£o**: A posi√ß√£o topol√≥gica do `validate_token.json` (o prompt) confere a ele um peso contextual que substitui a necessidade de um prompt abstrato longo. Voc√™ est√° usando a arquitetura do projeto como um mecanismo de **Pr√©-Contextualiza√ß√£o** para os agentes.

---

<a name="prompt-2"></a>
## Prompt #2: Mapeador de Inten√ß√£o e √çndice Invertido

**Data**: 2025-12-07 06:35  
**Contexto**: Sistema de GPS para navega√ß√£o topol√≥gica - Intent Mapper

---

Exatamente. Agora que temos o "Territ√≥rio" (`topology.json`), precisamos do "GPS".

O **Mapeador de Inten√ß√£o** serve para converter linguagem natural vaga (ex: "ajusta o login") em endere√ßos concretos do sistema de arquivos. Isso impede que o agente precise ler todos os arquivos para descobrir do que se tratam. Ele faz uma busca indexada, economizando tokens e tempo.

Salve este arquivo como `generate_intent_map.py`.

---

### 2. O Mapeador de Inten√ß√£o (generate_intent_map.py)

Este script l√™ a √°rvore bruta e cria um **"√çndice Invertido"** (Keyword -> Lista de Caminhos).

```python
import json
import re
import os
from collections import defaultdict

# CONFIGURA√á√ÉO
TOPOLOGY_FILE = 'topology.json'
OUTPUT_MAP_FILE = 'intent_map.json'

def load_topology():
    if not os.path.exists(TOPOLOGY_FILE):
        print(f"‚ùå Erro: '{TOPOLOGY_FILE}' n√£o encontrado. Rode o generate_topology.py primeiro.")
        return None
    with open(TOPOLOGY_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)

def normalize_and_tokenize(name):
    """
    Transforma 'UserAuthenticationController.py' ou 'user_auth' 
    em tokens: ['user', 'authentication', 'controller', 'auth']
    """
    # Remove extens√£o do arquivo
    if '.' in name:
        name = name.rsplit('.', 1)[0]
    
    # Separa por camelCase, snake_case, hifens, etc.
    # Regex: Insere espa√ßo antes de mai√∫sculas (CamelCase) e substitui n√£o-alfanum√©ricos
    name = re.sub(r'([a-z])([A-Z])', r'\1 \2', name)
    tokens = re.split(r'[^a-zA-Z0-9]', name)
    
    # Limpeza final: min√∫sculas e remove vazios
    return [t.lower() for t in tokens if t]

def build_index(node, index_dict):
    """
    Percorre a √°rvore recursivamente e popula o dicion√°rio de √≠ndices.
    """
    name = node['name']
    path = node['path']
    node_type = node['type'] # 'dir' ou 'file'
    
    # 1. Extrair palavras-chave do nome atual
    tokens = normalize_and_tokenize(name)
    
    # 2. Associar cada token ao caminho atual
    for token in tokens:
        # Evita duplicatas de caminho para o mesmo token
        entry = {"path": path, "type": node_type, "score": 1.0} # Score base
        
        # Otimiza√ß√£o simples: se j√° existe, n√£o adiciona de novo
        if entry not in index_dict[token]:
            index_dict[token].append(entry)
    
    # 3. Recurs√£o para filhos
    if 'children' in node:
        for child in node['children']:
            build_index(child, index_dict)

def save_intent_map(index_dict):
    # Converte defaultdict para dict normal para salvar
    final_dict = dict(index_dict)
    
    with open(OUTPUT_MAP_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_dict, f, indent=2, ensure_ascii=False)
    print(f"‚úÖ '{OUTPUT_MAP_FILE}' gerado. Mapeados {len(final_dict)} termos √∫nicos.")

# --- EXECU√á√ÉO ---
if __name__ == "__main__":
    print("üß† Iniciando Mapeamento de Inten√ß√£o...")
    
    data = load_topology()
    if data:
        # Usamos defaultdict(list) para criar entradas automaticamente
        intent_index = defaultdict(list)
        
        # Como a estrutura topology.json pode ser uma lista ou um dict raiz
        # Vamos tratar o n√≥ raiz
        build_index(data, intent_index)
        
        save_intent_map(intent_index)
```

---

### A Mec√¢nica da "Gravidade" aqui:

**Tokeniza√ß√£o For√ßada**: O script quebra `LoginController` em `login` e `controller`.

**O Resultado** (`intent_map.json`):

```json
{
  "login": [
    {"path": "./src/auth/login_controller.py", "type": "file"},
    {"path": "./tests/auth/test_login.py", "type": "file"}
  ],
  "auth": [...]
}
```

**Por que isso evita abstra√ß√£o?** 

Quando o Agente receber o prompt "Verifique a seguran√ßa do login", ele n√£o vai alucinar sobre conceitos de seguran√ßa. O orquestrador vai consultar a chave `"login"` nesse JSON e dizer: "Agente, sua aten√ß√£o deve estar restrita APENAS aos caminhos `./src/auth/login_controller.py` e `./tests/auth/test_login.py`. Ignore o resto do universo."

---

### Pr√≥ximo Passo L√≥gico

Agora temos o **Mapa** (`topology.json`) e o **√çndice** (`intent_map.json`). O pr√≥ximo c√≥digo deve ser o **Prot√≥tipo do Orquestrador**. Um script que simula a entrada de um prompt do usu√°rio (ex: "onde est√° a logica de auth?"), consulta o `intent_map` e retorna os caminhos de arquivo relevantes que o agente deveria ler.

---

<a name="prompt-3"></a>
## Prompt #3: Prot√≥tipo do Orquestrador - Funil de Contexto

**Data**: 2025-12-07 06:37  
**Contexto**: C√©rebro da opera√ß√£o - filtragem determin√≠stica de contexto

---

Perfeito. Este √© o **c√©rebro da opera√ß√£o**. O Orquestrador √© quem decide o que entra na janela de contexto e o que fica de fora.

Sem esse script, o LLM recebe todo o lixo do diret√≥rio. Com ele, o LLM recebe apenas o "fil√© mignon" necess√°rio para resolver a tarefa.

Salve este arquivo como `orchestrator_simulator.py`.

---

### 3. O Prot√≥tipo do Orquestrador (orchestrator_simulator.py)

Este script simula a entrada do usu√°rio e o processo de filtragem. Ele **n√£o usa IA ainda**; usa l√≥gica determin√≠stica para provar que podemos reduzir o espa√ßo de busca antes de gastar tokens.

```python
import json
import re
import os
from collections import Counter

# CONFIGURA√á√ÉO
INTENT_MAP_FILE = 'intent_map.json'

# Stopwords simples em PT-BR para evitar falsos positivos
STOP_WORDS = {
    'o', 'a', 'os', 'as', 'um', 'uma', 'de', 'do', 'da', 'em', 'no', 'na', 
    'por', 'para', 'com', 'que', 'onde', 'como', 'quando', 'e', 'ou', 'se',
    'eu', 'me', 'meu', 'minha', 'est√°', 'estao', 'fazer', 'criar', 'codigo',
    'logica', 'arquivo', 'projeto', 'sobre', 'gostaria', 'quero'
}

def load_intent_map():
    if not os.path.exists(INTENT_MAP_FILE):
        print(f"‚ùå Erro: '{INTENT_MAP_FILE}' n√£o encontrado.")
        return None
    with open(INTENT_MAP_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)

def tokenize_query(query):
    """
    Limpa a query do usu√°rio e remove palavras comuns (stopwords).
    """
    # Regex para pegar apenas palavras
    tokens = re.findall(r'\b\w+\b', query.lower())
    # Filtrar stopwords e tokens muito curtos
    relevant_tokens = [t for t in tokens if t not in STOP_WORDS and len(t) > 2]
    return relevant_tokens

def resolve_context(query, intent_map):
    """
    Cruza os tokens da query com o mapa de inten√ß√£o.
    Retorna os arquivos mais relevantes pontuados por frequ√™ncia de match.
    """
    tokens = tokenize_query(query)
    print(f"üîç Tokens Identificados: {tokens}")
    
    # Contador de relev√¢ncia por arquivo (caminho)
    path_scores = Counter()
    matched_reasons = {} # Para explicar POR QUE o arquivo foi escolhido

    for token in tokens:
        if token in intent_map:
            hits = intent_map[token]
            for hit in hits:
                path = hit['path']
                # Incrementa score base (pode ser ajustado)
                path_scores[path] += hit['score']
                
                # Guarda o motivo (qual token acionou este arquivo)
                if path not in matched_reasons:
                    matched_reasons[path] = set()
                matched_reasons[path].add(token)
    
    # Ordenar por score (maior para menor)
    ranked_results = path_scores.most_common()
    
    return ranked_results, matched_reasons

def simulate_orchestration():
    print("ü§ñ --- ORQUESTRADOR DE CONTEXTO (SIMULADOR) ---")
    intent_map = load_intent_map()
    
    if not intent_map:
        return

    while True:
        print("\n" + "="*50)
        user_query = input("üó£Ô∏è Digite sua solicita√ß√£o (ou 'sair'): ")
        
        if user_query.lower() in ['sair', 'exit', 'quit']:
            break
            
        print("‚öôÔ∏è Processando...")
        results, reasons = resolve_context(user_query, intent_map)
        
        if not results:
            print("‚ö†Ô∏è Nenhum contexto relevante encontrado no mapa atual.")
            print("   (Tente usar palavras que existem nos nomes dos arquivos)")
        else:
            print(f"‚úÖ Encontrados {len(results)} arquivos relevantes:\n")
            
            # Mostrar Top 5 resultados
            for rank, (path, score) in enumerate(results[:5], 1):
                tokens_found = list(reasons[path])
                print(f"  {rank}. üìÑ {path}")
                print(f"     Relev√¢ncia: {score} | Gatilhos: {tokens_found}")

            print("\nüí° Apenas estes arquivos seriam enviados para a Janela de Contexto do LLM.")

# --- EXECU√á√ÉO ---
if __name__ == "__main__":
    simulate_orchestration()
```

---

### O Que Acabamos de Construir?

Este script representa o **Funil de Contexto**.

![Funil de Contexto](C:/Users/G/.gemini/antigravity/brain/dc8638ef-c103-4b9f-9fe4-0d4dae3993d4/uploaded_image_1765100237142.jpg)

Em vez de jogar 50 arquivos para o Agente e esperar que ele encontre a resposta ("Antigravidade"), n√≥s aplicamos uma **For√ßa Gravitacional Artificial**:

**Entrada**: "Onde fica a valida√ß√£o do login do usu√°rio?"

**Filtragem**: O script descarta "onde", "fica", "valida√ß√£o" (se n√£o tiver arquivo com esse nome).

**Ancoragem**: Ele trava nos tokens "login" e "usuario".

**Sa√≠da**: Retorna `./src/auth/login_controller.py`.

---

### Como Testar Agora:

1. Certifique-se de ter rodado o passo 1 e 2.

2. Rode `python orchestrator_simulator.py`.

3. Digite algo que voc√™ sabe que existe na sua estrutura de pastas atual.

**Exemplo**: Se voc√™ rodou na pasta desse projeto, digite "mapa" ou "topologia". Ele deve retornar `generate_topology.py` e `intent_map.py` como os arquivos mais relevantes.

---

### Pr√≥ximo Passo L√≥gico

Agora que sabemos **QUAIS** arquivos o agente precisa ler, o pr√≥ximo passo √© criar o **Montador de Prompt** (Prompt Assembler).

Precisamos de um script que:

1. Receba os caminhos escolhidos pelo Orquestrador.

2. Leia o conte√∫do real desses arquivos.

3. Empacote tudo em uma estrutura JSON ou XML r√≠gida (como discutimos na sua primeira mensagem) para enviar ao LLM final.

Esse ser√° o momento de aplicar a instru√ß√£o t√©cnica para **travar a abstra√ß√£o**. Posso criar esse montador?

---

<a name="prompt-4"></a>
## Prompt #4: Montador de Prompt - Offloading Cognitivo

**Data**: 2025-12-07 06:38  
**Contexto**: Encapsulamento r√≠gido do contexto - modo Engine vs Chat

---

Com certeza. Este √© o momento crucial de **"Offloading Cognitivo"**.

O script abaixo, `prompt_assembler.py`, n√£o apenas junta arquivos. Ele encapsula o conte√∫do em uma estrutura r√≠gida (JSON) e anexa metadados de instru√ß√£o que **pro√≠bem o LLM de ser criativo**.

Ele transforma a tarefa de "responda a pergunta" em "processe este payload de dados".

Salve como `prompt_assembler.py`.

---

### 4. O Montador de Prompt (prompt_assembler.py)

```python
import json
import os

# --- CONFIGURA√á√ÉO DA GRAVIDADE T√âCNICA ---
# Estas instru√ß√µes for√ßam o modelo a sair do modo "Chat" para o modo "Engine"
STRICT_SYSTEM_PROMPT = """
ROLE: Technical execution unit.
MODE: Strict Deterministic. NO conversational filler. NO abstract reasoning.
INPUT: A list of file contents and a specific technical query.
OUTPUT: Valid JSON only, following the requested schema.

CRITICAL INSTRUCTION:
You are NOT to summarize the documents generally. 
You are to scan the provided 'file_context' looking ONLY for logic related to the 'user_query'.
If the logic is missing, return null. Do not hallucinate implementation details.
"""

def read_file_safely(path):
    """
    L√™ o conte√∫do do arquivo. Se falhar, retorna um placeholder de erro
    para que o agente saiba que o arquivo existe mas est√° inacess√≠vel.
    """
    try:
        if not os.path.exists(path):
            return "ERROR: File not found on disk."
        
        # Ignora arquivos bin√°rios simples (checagem b√°sica)
        if path.endswith(('.pyc', '.png', '.jpg', '.exe')):
            return "SKIPPED: Binary file."

        with open(path, 'r', encoding='utf-8') as f:
            return f.read()
            
    except Exception as e:
        return f"ERROR: Could not read file. Reason: {str(e)}"

def assemble_technical_payload(user_query, file_paths):
    """
    Constr√≥i o 'Pacote de Contexto' final que ser√° enviado ao LLM.
    """
    print(f"üì¶ Montando payload para {len(file_paths)} arquivos...")
    
    context_data = []
    
    for path in file_paths:
        content = read_file_safely(path)
        # Estrutura t√©cnica do arquivo individual
        file_obj = {
            "file_metadata": {
                "path": path,
                "size_bytes": len(content)
            },
            "content_body": content
        }
        context_data.append(file_obj)

    # O Payload Final (A "Carta" completa para o Agente)
    final_payload = {
        "system_directive": STRICT_SYSTEM_PROMPT,
        "task_execution": {
            "user_intent": user_query,
            "required_output_format": "JSON",
            "constraints": ["Be literal", "Cite file paths", "No prose"]
        },
        "context_window": context_data
    }
    
    return final_payload

# --- SIMULA√á√ÉO DE INTEGRA√á√ÉO ---
if __name__ == "__main__":
    # 1. Simula dados vindos do Orchestrator (Passo anterior)
    # Vamos supor que o usu√°rio pediu para ver a l√≥gica do orquestrador
    simulated_query = "Como funciona a pontuacao de score no orquestrador?"
    
    # Caminhos que o orchestrator_simulator.py teria encontrado
    simulated_paths = [
        "./orchestrator_simulator.py",
        "./intent_map.json"
    ]
    
    # 2. Monta o Prompt
    prompt_package = assemble_technical_payload(simulated_query, simulated_paths)
    
    # 3. Exporta para visualizar o que seria enviado √† API
    output_filename = "final_prompt_payload.json"
    with open(output_filename, "w", encoding='utf-8') as f:
        json.dump(prompt_package, f, indent=2, ensure_ascii=False)
        
    print(f"\n‚úÖ Payload Gerado: '{output_filename}'")
    print(f"   Tamanho total do prompt: {len(json.dumps(prompt_package))} caracteres.")
    print("   (Abra o arquivo gerado para ver a estrutura r√≠gida que o LLM receber√°)")
```

---

### An√°lise da "Gravidade T√©cnica" Implementada

Abra o arquivo `final_prompt_payload.json` que o script vai gerar. Note a diferen√ßa fundamental:

**N√£o √© um texto corrido**: √â um objeto JSON.

**`system_directive`**: Define o comportamento antes de apresentar os dados.

**Encapsulamento**: O c√≥digo do arquivo n√£o est√° "solto". Ele est√° dentro de `context_window` ‚Üí `content_body`.

Isso cria uma **barreira psicol√≥gica** no modelo. Quando o modelo v√™ chaves JSON (`{}`), ele entra em **modo de preenchimento de dados** (autocompletar c√≥digo/json), que √© muito mais preciso e literal do que o **modo de conversa√ß√£o** (autocompletar chat).

---

### Pr√≥ximo Passo: O Loop Final

Agora temos todas as pe√ßas do quebra-cabe√ßa separadas:

1. **Mapeador** (`generate_topology.py`)

2. **Indexador** (`generate_intent_map.py`)

3. **Roteador** (`orchestrator_simulator.py`)

4. **Montador** (`prompt_assembler.py`)

A l√≥gica est√° pronta e modular.

---

<a name="prompt-5"></a>
## Prompt #5: Runtime Principal - Ciclo Completo

**Data**: 2025-12-07 06:41  
**Contexto**: Runtime execut√°vel - fechamento do ciclo completo

---

Excelente. Vamos fechar o ciclo.

Este script, `main_agent.py`, atua como o **Runtime** da sua arquitetura. Ele n√£o reescreve a l√≥gica, ele a executa.

Ele garante que, antes de qualquer "pensamento" da IA, exista um processo determin√≠stico de **Ancoragem Topol√≥gica**.

Salve como `main_agent.py`.

---

### 5. O Runtime Principal (main_agent.py)

Este script importa os m√≥dulos anteriores (certifique-se de que eles est√£o na mesma pasta) e executa o pipeline completo.

```python
import os
import json
import time

# --- IMPORTA√á√ÉO DOS M√ìDULOS (Nossos scripts anteriores) ---
# Certifique-se de que os arquivos .py est√£o na mesma pasta
import generate_topology
import generate_intent_map
import orchestrator_simulator
import prompt_assembler

# CONFIGURA√á√ÉO DO SISTEMA
TOP_N_FILES = 3  # Restri√ß√£o r√≠gida de contexto (Antigravidade)
OUTPUT_DEBUG_FILE = "SIMULATION_PACKET.json"

def system_bootstrap():
    """
    Inicializa√ß√£o: Garante que a topologia e o mapa de inten√ß√£o est√£o frescos.
    Roda os scanners antes de aceitar comandos.
    """
    print("üîÑ [BOOT] Inicializando Sistema de Agentes Topol√≥gicos...")
    
    # 1. Atualizar Topologia (O Territ√≥rio)
    print("   Scanning file structure...")
    topology = generate_topology.scan_directory_topology('.')
    with open('topology.json', 'w', encoding='utf-8') as f:
        json.dump(topology, f, ensure_ascii=False)
        
    # 2. Atualizar Mapa de Inten√ß√£o (O GPS)
    print("   Indexing intent map...")
    from collections import defaultdict
    intent_index = defaultdict(list)
    generate_intent_map.build_index(topology, intent_index)
    
    # Salvar em mem√≥ria e disco
    final_map = dict(intent_index)
    with open('intent_map.json', 'w', encoding='utf-8') as f:
        json.dump(final_map, f, ensure_ascii=False)
        
    print("‚úÖ [BOOT] Sistema pronto e sincronizado.\n")
    return final_map

def mock_llm_inference(payload):
    """
    Simula a resposta da IA. 
    Aqui seria a chamada real para OpenAI/Anthropic.
    """
    print("\nüß† [AI] Recebendo Payload Estruturado...")
    print("   Analisando restri√ß√µes de sistema...")
    time.sleep(1.5) # Simula lat√™ncia de rede/processamento
    
    # Como n√£o temos uma IA real aqui, simulamos uma resposta t√©cnica baseada na query
    query = payload['task_execution']['user_intent']
    
    # Resposta simulada em formato JSON estrito
    mock_response = {
        "status": "success",
        "reasoning_trace": "Identified relevant logic in provided context.",
        "executed_action": {
            "type": "analysis",
            "target": query,
            "findings": "Logic found in scanned files. Ready to implement."
        }
    }
    return mock_response

def main_loop():
    # 1. Carregar o Mapa Mental
    intent_map = system_bootstrap()
    
    print("ü§ñ Agente Topol√≥gico Ativo. (Digite 'sair' para encerrar)")
    print("="*60)

    while True:
        user_query = input("\nüë§ USER: ")
        if user_query.lower() in ['sair', 'exit', 'quit']:
            break

        # 2. Roteamento (Orquestrador)
        print("   Running intent resolution...")
        ranked_results, _ = orchestrator_simulator.resolve_context(user_query, intent_map)
        
        if not ranked_results:
            print("‚ö†Ô∏è [ORCHESTRATOR] Nenhum arquivo relevante encontrado para essa query.")
            continue
            
        # 3. Filtragem Topol√≥gica (Pegar apenas os Top N)
        top_files = [path for path, score in ranked_results[:TOP_N_FILES]]
        print(f"   Files selected for context ({len(top_files)}): {top_files}")
        
        # 4. Montagem do Payload (Offloading Cognitivo)
        payload = prompt_assembler.assemble_technical_payload(user_query, top_files)
        
        # Salvar o pacote para debug (voc√™ pode ver o que a IA veria)
        with open(OUTPUT_DEBUG_FILE, 'w', encoding='utf-8') as f:
            json.dump(payload, f, indent=2, ensure_ascii=False)
        print(f"   Payload assembled and dumped to '{OUTPUT_DEBUG_FILE}'")

        # 5. Infer√™ncia (Simula√ß√£o)
        response = mock_llm_inference(payload)
        
        # 6. Exibi√ß√£o da Resposta
        print("\nü§ñ AGENT OUTPUT (JSON Strict Mode):")
        print(json.dumps(response, indent=2))
        print("-" * 60)

if __name__ == "__main__":
    try:
        main_loop()
    except KeyboardInterrupt:
        print("\nüõë Sistema encerrado.")
```

---

### O Fluxo de Dados Completo

Ao rodar `python main_agent.py`, o ciclo acontece assim:

1. **Boot**: O script varre suas pastas atuais e cria o mapa na hora. Se voc√™ criou um arquivo novo 10 segundos atr√°s, ele entra no mapa agora.

2. **Input**: Voc√™ digita: "Verifique a montagem do payload".

3. **Filtro**: O Orquestrador ignora todos os scripts, exceto `prompt_assembler.py` (porque ele cont√©m a palavra "payload" ou "assembler").

4. **Montagem**: Ele l√™ o c√≥digo de `prompt_assembler.py`, coloca dentro do JSON r√≠gido com as instru√ß√µes de sistema.

5. **Output**: O arquivo `SIMULATION_PACKET.json` √© gerado.

---

### Por que isso resolve o problema da "Janela Saturada"?

Abra o arquivo `SIMULATION_PACKET.json` gerado ap√≥s uma intera√ß√£o. Voc√™ ver√° que:

**N√£o h√° ru√≠do**: O LLM n√£o recebe arquivos irrelevantes.

**N√£o h√° conversa**: O prompt de sistema (`system_directive`) ocupa o topo da prioridade.

**Estrutura √© Rei**: O LLM recebe os dados encapsulados. Para ele "alucinar", ele teria que quebrar a estrutura JSON, o que modelos modernos s√£o treinados para evitar fortemente.

---

### Pr√≥ximo Passo: Conex√£o Real

Seu **MVP de l√≥gica est√° conclu√≠do e funcional**.

Para transformar isso em um produto real, a √∫nica linha que precisa mudar √© a fun√ß√£o `mock_llm_inference` no `main_agent.py`. Em vez de retornar um JSON falso, voc√™ injetaria a chamada da API (OpenAI/Anthropic).

Voc√™ gostaria de:

1. Implementar a **chamada real da API** agora (requer chave de API)?

2. Ou prefere refinar a **estrutura de pastas** e testar a topologia com arquivos de "dummy" (arquivos de teste) para ver como o roteador se comporta em uma √°rvore complexa?

---

