# üîÑ Feedback Collector

**Module**: `core/loop/feedback_collector.py`
**Class**: `ActionFeedbackCollector`

---

## Prop√≥sito

O `ActionFeedbackCollector` √© respons√°vel por fechar o ciclo de aprendizado, transformando o resultado bruto das a√ß√µes (`ActionResult`) em sinais de treinamento (`TrainingFeedback`) para a camada neural (Neural Learner).

Ele atua como um "cr√≠tico" que avalia se uma a√ß√£o foi bem-sucedida e qual a magnitude de sua recompensa.

---

## L√≥gica de Recompensa (Reward Shaping)

A fun√ß√£o `_calculate_reward` implementa a heur√≠stica de recompensa:

1.  **Falha na A√ß√£o**: Reward fixo negativo (`-0.5`).
2.  **Sucesso sem Evid√™ncia**: Reward neutro (`0.0`).
3.  **Sucesso com Evid√™ncia**: Base positiva (`0.5`) + Proporcional √† evid√™ncia encontrada (at√© `1.0`).
4.  **B√¥nus de Conex√£o**: Adicional de `+0.3` se a a√ß√£o gerou novas arestas no grafo causal.

### F√≥rmula
```python
reward = base_reward + (evidence_score * 0.5) + connection_bonus
```
Limitado por `min_reward` e `max_reward`.

---

## Estrutura de Dados

### `TrainingFeedback`

Objeto padronizado enviado para o `IncrementalLearner`:

- `embeddings`: Lista de vetores das evid√™ncias encontradas (para treino contrastivo).
- `reward_signal`: Scalar float indicando qualidade da a√ß√£o (-1.0 a +1.0).
- `should_learn`: Booleano indicando se o feedback √© significativo o suficiente para disparar backprop.
- `source_action_type`: Tipo da a√ß√£o que gerou o feedback.

---

## API Reference

### `collect`

```python
def collect(self, action_result: Dict) -> TrainingFeedback
```

Processa o resultado da a√ß√£o, calcula reward e extrai embeddings se houver topologia dispon√≠vel.

### `get_stats`

```python
def get_stats(self) -> Dict
```

Retorna m√©tricas acumuladas:
- Total de feedbacks coletados.
- Taxa de feedbacks positivos.
- Reward m√©dio (janela m√≥vel).

---

## Exemplo de Integra√ß√£o

```python
# No loop principal:
result = executor.execute(hypothesis)
feedback = collector.collect(result.to_dict())

if feedback.should_learn:
    learner.train(feedback)
```
