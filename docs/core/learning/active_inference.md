# üß† Active Inference

**Module**: `core/learning/active_inference.py`  
**Lines of Code**: 1486  
**Purpose**: Agente de Active Inference para sele√ß√£o de a√ß√µes epist√™micas

---

## üéØ Overview

O Active Inference implementa um **agente cognitivo aut√¥nomo** baseado no framework de Karl Friston. O agente mant√©m cren√ßas (beliefs) sobre o estado do conhecimento, tem prefer√™ncias sobre estados desejados, e seleciona a√ß√µes que minimizam a energia livre esperada (Expected Free Energy).

Este √© o m√≥dulo que d√° **autonomia** ao Alexandria - permite que o sistema decida proativamente o que explorar, quais gaps investigar, e como expandir o conhecimento autonomamente.

### Conceito Chave: Expected Free Energy

```
G(œÄ) = Risk + Ambiguity
     = D_KL[Q(o|œÄ)||P(o)] + E_Q[H(o|s,œÄ)]

Risk: Qu√£o longe das prefer√™ncias
Ambiguity: Incerteza sobre resultados
```

O agente escolhe a√ß√µes que **minimizam G** - ou seja, que levam a estados preferidos (baixo risk) e que reduzem incerteza (baixa ambiguity).

---

## üèóÔ∏è Architecture

```mermaid
graph TB
    subgraph Core["Active Inference Agent"]
        B[Beliefs Q(s)]
        P[Preferences P(o)]
        GM[Generative Model]
    end
    
    subgraph Actions["Tipos de A√ß√£o"]
        A1[QUERY_SEARCH]
        A2[EXPLORE_CLUSTER]
        A3[FOLLOW_CONNECTION]
        A4[DEEPEN_TOPIC]
        A5[BRIDGE_CONCEPTS]
        A6[FILL_GAP]
    end
    
    subgraph Planning["Sele√ß√£o de A√ß√£o"]
        EFE[Expected Free Energy G]
        PI[Policy œÄ]
    end
    
    B --> EFE
    P --> EFE
    GM --> EFE
    EFE --> PI
    PI --> Actions
    
    style Core fill:#E91E63,color:#fff
    style Planning fill:#2196F3,color:#fff
```

---

## üîç Modelo Generativo

O `GenerativeModel` encapsula a din√¢mica do mundo:

```python
# P(s_t+1 | s_t, a_t): Transi√ß√£o de estado dado a√ß√£o
def predict_next_state(self, current_state, action):
    transition_matrix = self.transition_models[action.action_type]
    mean = transition_matrix @ current_state
    return mean, precision

# P(o_t | s_t): Observa√ß√£o esperada dado estado
def predict_observation(self, state):
    return self.observation_model @ state
```

### Aprendizado do Modelo

```python
# Œî A = lr * (s_t+1 - A @ s_t) @ s_t.T
def update_from_experience(self, prev_state, action, next_state):
    prediction = self.transition_models[action.action_type] @ prev_state
    error = next_state - prediction
    self.transition_models[action.action_type] += lr * np.outer(error, prev_state)
```

---

## üîç Sele√ß√£o de A√ß√£o

### Expected Free Energy

```python
def _compute_expected_free_energy(self, action, current_state, horizon):
    """
    G(œÄ) = Œ£_t [Risk_t + Ambiguity_t]
    """
    total_G = 0
    state = current_state
    
    for t in range(horizon):
        # Prediz pr√≥ximo estado
        next_state, precision = self.generative_model.predict_next_state(state, action)
        
        # Risk: dist√¢ncia das prefer√™ncias
        risk = self._compute_risk(next_state)
        
        # Ambiguity: incerteza da predi√ß√£o
        ambiguity = self._compute_ambiguity(precision)
        
        total_G += risk + ambiguity
        state = next_state
    
    return total_G
```

### Planejamento

```python
def plan(self, horizon=5, context=None):
    """Seleciona melhor a√ß√£o via Monte Carlo"""
    
    candidate_actions = self._generate_candidate_actions(context)
    
    for action in candidate_actions:
        G = self._compute_expected_free_energy(action, self.current_state, horizon)
        action.expected_information_gain = -G  # Menor G = maior ganho
    
    # Softmax sobre -G para sele√ß√£o probabil√≠stica
    return self._softmax_select(candidate_actions)
```

---

## üîó Inter-Module Communication

### **Depends On**:

```mermaid
graph LR
    AI[Active Inference] --> MH[Meta-Hebbian]
    AI --> PC[Predictive Coding]
    AI --> FE[Free Energy]
    
    style AI fill:#E91E63,color:#fff
```

**1. Meta-Hebbian** (`meta_hebbian.py`)
- **Purpose**: Plasticidade adaptativa
- **Call**: `meta_hebbian.compute_weight_update()`
- **When**: Ap√≥s a√ß√£o bem-sucedida

**2. Free Energy** (`free_energy.py`)
- **Purpose**: M√©tricas de minimiza√ß√£o
- **Call**: `free_energy.compute()`
- **When**: Avalia√ß√£o de estados

### **Used By**:

```mermaid
graph LR
    IL[Integration Layer] --> AI[Active Inference]
    AA[Action Agent] --> AI
    
    style AI fill:#E91E63,color:#fff
```

---

## üìä Core Classes

### ActionType

```python
class ActionType(Enum):
    QUERY_SEARCH = auto()       # Buscar documentos
    EXPLORE_CLUSTER = auto()    # Explorar cluster de conceitos
    FOLLOW_CONNECTION = auto()  # Seguir conex√£o existente
    DEEPEN_TOPIC = auto()       # Aprofundar em t√≥pico
    BRIDGE_CONCEPTS = auto()    # Conectar conceitos distantes
    FILL_GAP = auto()           # Preencher gap de conhecimento
    CONSOLIDATE = auto()        # Consolidar aprendizado
    REST = auto()               # N√£o fazer nada
```

### Action

```python
@dataclass
class Action:
    action_type: ActionType
    target: str                          # Alvo da a√ß√£o
    parameters: Dict[str, Any]
    expected_information_gain: float     # Ganho esperado
    expected_risk: float
    priority: float
```

### Belief

```python
@dataclass
class Belief:
    concept_id: str
    mean: np.ndarray          # Estado estimado
    precision: np.ndarray     # Inverso da vari√¢ncia
    last_updated: int
    observation_count: int
    
    @property
    def uncertainty(self):
        return 1.0 / np.mean(self.precision)
```

### Preference

```python
@dataclass
class Preference:
    topic_weights: Dict[str, float]  # Peso por t√≥pico
    novelty_preference: float = 0.5   # 0=conservador, 1=explorador
    depth_preference: float = 0.5     # 0=amplo, 1=profundo
    connection_preference: float = 0.5
```

---

## üéØ Use Cases

### 1. Explora√ß√£o Aut√¥noma

```python
from core.learning.active_inference import ActiveInferenceAgent, ActiveInferenceConfig

config = ActiveInferenceConfig(
    planning_horizon=5,
    novelty_bonus=0.3
)
agent = ActiveInferenceAgent(config)

# Agente decide pr√≥xima a√ß√£o
action = agent.plan(context={"current_topic": "neural networks"})
print(f"A√ß√£o escolhida: {action.action_type} ‚Üí {action.target}")
```

### 2. Detec√ß√£o de Gaps

```python
# Identifica gaps no conhecimento
gaps = agent.detect_knowledge_gaps()

for gap in gaps[:5]:
    print(f"Gap: {gap.description} (priority: {gap.priority:.2f})")
```

### 3. Ciclo Completo

```python
# Loop aut√¥nomo de aprendizado
while agent.should_continue():
    action = agent.plan()
    observation = execute_action(action)  # Externo
    agent.update_beliefs(observation)
    agent.learn_from_experience(action, observation)
```

---

## üìà Performance

| Operation | Time | Notes |
|-----------|------|-------|
| **plan()** | ~50ms | 5-step horizon |
| **update_beliefs()** | ~5ms | Single belief |
| **detect_gaps()** | ~200ms | Full scan |
| **learn_from_experience()** | ~10ms | Model update |

---

## ‚öôÔ∏è Configuration

```python
@dataclass
class ActiveInferenceConfig:
    state_dim: int = 64              # Dimens√£o do estado interno
    action_embedding_dim: int = 32   # Embedding das a√ß√µes
    planning_horizon: int = 5        # Passos de lookahead
    num_action_samples: int = 20     # Candidatos por sele√ß√£o
    temperature: float = 1.0         # Softmax temperature
    risk_weight: float = 1.0         # Peso do risco
    ambiguity_weight: float = 1.0    # Peso da ambiguidade
    novelty_bonus: float = 0.3       # B√¥nus para novidade
    learning_rate: float = 0.01      # LR do modelo
    belief_decay: float = 0.99       # Decay dos beliefs
```

---

## üîÆ Future Enhancements

- [ ] Hierarchical Active Inference (multi-scale planning)
- [ ] Meta-learning de prefer√™ncias baseado em feedback
- [ ] Integra√ß√£o com reinforcement learning
- [ ] A√ß√µes cont√≠nuas (n√£o apenas discretas)

---

**Last Updated**: 2025-12-07  
**Version**: 1.0  
**Status**: Production
