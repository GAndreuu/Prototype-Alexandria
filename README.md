# üèõÔ∏è Alexandria - Local AI Synthesis

<div align="center">

![Status](https://img.shields.io/badge/status-production-success?style=for-the-badge)
![Python](https://img.shields.io/badge/python-3.10+-blue?style=for-the-badge&logo=python)
![License](https://img.shields.io/badge/license-MIT-green?style=for-the-badge)

**S√≠ntese Local de IA atrav√©s de Aprendizado N√£o-Supervisionado**

*A cognitive architecture for local, self-improving AI systems*

[Objetivos](#-objetivos) ‚Ä¢ [Como Funciona](#-como-funciona) ‚Ä¢ [Teorias](#-fundamentos-te√≥ricos) ‚Ä¢ [Documenta√ß√£o](#-documenta√ß√£o)

</div>

---

## üéØ Objetivos

Alexandria busca criar uma **s√≠ntese local de IA** atrav√©s de:

1. **Autonomia Total**: Zero depend√™ncias de servi√ßos cloud ou APIs externas
2. **Aprendizado Cont√≠nuo**: Sistema que se aperfei√ßoa com cada observa√ß√£o
3. **Racioc√≠nio Emergente**: Intelig√™ncia que surge de processos bio-inspirados simples
4. **Transpar√™ncia**: Visualiza√ß√£o completa dos processos internos de decis√£o
5. **Efici√™ncia**: Compress√£o neural radical (96%) para operar localmente

**Vis√£o**: Uma IA aut√¥noma e auto-aperfei√ßoante que roda na sua m√°quina, sem cloud, sem censura, sem custo operacional.

---

## üìñ A Hist√≥ria do Sistema

Alexandria come√ßou como um experimento em **composicionalidade sem√¢ntica**: ser√° que c√≥digos discretos podem raciocinar como embeddings cont√≠nuos?

A resposta foi surpreendente: **sim, mas de forma fundamentalmente diferente**.

### Evolu√ß√£o

1. **Fase I**: RAG b√°sico com LanceDB
2. **Fase II**: VQ-VAE para compress√£o neural
3. **Fase III**: Mycelial Network (Hebbian learning)
4. **Fase IV**: Nemesis Core (Active Inference + Predictive Coding)
5. **Fase Atual**: Integra√ß√£o completa e otimiza√ß√£o

Hoje, Alexandria √© um sistema cognitivo completo que combina 6 paradigmas te√≥ricos em uma arquitetura unificada.

---

## üß† Como Funciona

### Arquitetura Completa

```mermaid
graph TB
    subgraph Input["üì• INTERFACE DE ENTRADA"]
        A[Documentos/Imagens]
        B[Queries do Usu√°rio]
    end
    
    subgraph Processing["‚öôÔ∏è PROCESSAMENTO MULTIMODAL"]
        C[Sentence Transformer]
        D[V11 Vision Encoder]
        E[384D Embeddings]
    end
    
    subgraph Compression["ÔøΩÔ∏è COMPRESS√ÉO NEURAL"]
        F[VQ-VAE Monolith V13]
        G[Product Quantizer]
        H[4 bytes/chunk]
        I[4 heads √ó 256 codes]
    end
    
    subgraph Memory["üóÑÔ∏è MEM√ìRIA SEM√ÇNTICA"]
        J[LanceDB Vector Store]
        K[193k documents indexed]
    end
    
    subgraph Reasoning["üçÑ RACIOC√çNIO MICELAR"]
        L[Mycelial Network]
        M[638k Hebbian Connections]
        N[Activation Propagation]
    end
    
    subgraph Nemesis["üß¨ NEMESIS CORE"]
        O[Active Inference Agents]
        P[Predictive Coding]
        Q[Free Energy Minimization]
    end
    
    subgraph Intelligence["üéØ CAMADA DE INTELIG√äNCIA"]
        R[Abduction Engine]
        S[Causal Reasoning]
        T[Meta-Hebbian Plasticity]
    end
    
    subgraph Output["üì§ SA√çDA"]
        U[Respostas Enriquecidas]
        V[Hip√≥teses Geradas]
        W[A√ß√µes Aut√¥nomas]
    end
    
    A --> C
    A --> D
    B --> J
    C --> E
    D --> E
    E --> F
    F --> G
    G --> H
    H --> I
    I --> J
    J --> L
    L --> M
    M --> N
    N --> O
    O --> P
    P --> Q
    Q --> R
    R --> S
    S --> T
    T --> U
    T --> V
    T --> W
    
    style Input fill:#e3f2fd
    style Processing fill:#fff3e0
    style Compression fill:#f3e5f5
    style Memory fill:#e8f5e9
    style Reasoning fill:#fce4ec
    style Nemesis fill:#fff9c4
    style Intelligence fill:#e0f2f1
    style Output fill:#fce4ec
```

### Fluxo de Dados

```
1. INGEST√ÉO
   ‚îî‚îÄ> Document ‚Üí Chunking (~1000 chars) ‚Üí Embedding (384D)

2. COMPRESS√ÉO
   ‚îî‚îÄ> Embedding ‚Üí VQ-VAE ‚Üí 4 bytes (96% compression)

3. ARMAZENAMENTO
   ‚îî‚îÄ> LanceDB indexing + Mycelial observation

4. RACIOC√çNIO
   ‚îú‚îÄ> Semantic Search (LanceDB)
   ‚îú‚îÄ> Hebbian Propagation (Mycelial)
   ‚îî‚îÄ> Active Inference (Nemesis)

5. S√çNTESE
   ‚îú‚îÄ> Abduction (Gap detection)
   ‚îú‚îÄ> Causal Reasoning (Graph construction)
   ‚îî‚îÄ> Meta-Hebbian Plasticity (Self-optimization)

6. A√á√ÉO
   ‚îî‚îÄ> Response | Hypothesis | Autonomous Action
```

---

## ÔøΩ Fundamentos Te√≥ricos

Alexandria combina m√∫ltiplas teorias de neuroci√™ncia computacional e IA:

### 1. **Hebbian Learning** (Donald Hebb, 1949)
> *"Neurons that fire together, wire together"*

- **Aplica√ß√£o**: Mycelial Network aprende co-ocorr√™ncias de c√≥digos VQ-VAE
- **Vantagem**: N√£o-supervisionado, online, biologicamente plaus√≠vel
- **Status**: 638,130 conex√µes ativas, densidade <1%

### 2. **Free Energy Principle** (Karl Friston, 2010)
> Sistemas inteligentes minimizam surpresa variacional

- **Aplica√ß√£o**: Nemesis Core usa Active Inference para sele√ß√£o de a√ß√µes
- **Vantagem**: Framework unificado para percep√ß√£o, a√ß√£o e aprendizado
- **Status**: Operacional com overflow warnings esperados

### 3. **Predictive Coding** (Rao & Ballard, 1999)
> C√©rebros s√£o m√°quinas preditivas que minimizam erro

- **Aplica√ß√£o**: Hierarquia de 5 camadas prediz embeddings
- **Vantagem**: Compress√£o + predi√ß√£o em uma √∫nica arquitetura
- **Status**: 4 camadas constru√≠das (384‚Üí256‚Üí128‚Üí64‚Üí32)

### 4. **Vector Quantization** (VQ-VAE, van den Oord, 2017)
> Compress√£o neural via codebook discreto

- **Aplica√ß√£o**: Monolith V13 com 4 heads √ó 256 codes
- **Vantagem**: 96% compression mantendo reconstru√ß√£o (MSE 0.0021)
- **Status**: 255/256 c√≥digos ativos, Head 0 dominante (67%)

### 5. **Abductive Reasoning** (Charles Peirce, 1878)
> Infer√™ncia √† melhor explica√ß√£o

- **Aplica√ß√£o**: Detec√ß√£o autom√°tica de gaps e gera√ß√£o de hip√≥teses
- **Vantagem**: Expans√£o aut√¥noma de conhecimento
- **Status**: Gap detection operacional

### 6. **Meta-Learning** (Schmidhuber, 1987)
> Aprender a aprender

- **Aplica√ß√£o**: Meta-Hebbian ajusta taxas de plasticidade dinamicamente
- **Vantagem**: Auto-otimiza√ß√£o sem interven√ß√£o manual
- **Status**: Implementado, em teste

---

## üèóÔ∏è M√≥dulos Principais

### VQ-VAE (Neural Compression)
**Arquivo**: `core/reasoning/vqvae/`

- **Monolith V13**: 4 heads, 256 codes/head, 384D ‚Üí 4 bytes
- **Head Balance Regularization**: Previne colapso de codebook
- **Status**: ‚úÖ Produ√ß√£o (epoch 20, codebook 99.6% ativo)

### Mycelial Network (Hebbian Reasoning)
**Arquivo**: `core/reasoning/mycelial_reasoning.py`

- **638,130 conex√µes** aprendidas via Hebb
- **Propaga√ß√£o**: 3-5 steps para enriquecer queries
- **Status**: ‚úÖ Saud√°vel (densidade <1%, hubs emergentes)

### Nemesis Core (Active Inference)
**Arquivos**: `core/learning/`

- **Active Inference**: Scout, Judge, Weaver agents
- **Predictive Coding**: 5-layer hierarchical prediction
- **Free Energy**: Governan√ßa top-level
- **Status**: ‚úÖ Operacional (warnings num√©ricos esperados)

### LanceDB (Vector Storage)
**Arquivo**: `core/memory/storage.py`

- **193,502 documentos** indexados
- **Busca**: <50ms (p99) para top-10
- **Status**: ‚úÖ Operacional

---

## üìä M√©tricas de Performance

| Opera√ß√£o | Performance | Notas |
|:---|:---:|:---|
| **Indexa√ß√£o** | 1,000 chunks/s | Batch processing |
| **Busca Vetorial** | <50ms (p99) | Top-10 resultados |
| **Propaga√ß√£o Micelar** | <15ms | 3 steps |
| **Compress√£o VQ-VAE** | 96% | 384D ‚Üí 4 bytes |
| **Codebook Ativo** | 99.6% | 255/256 codes |
| **Conex√µes Hebbian** | 638,130 | Densidade <1% |

### Escalabilidade

| Documentos | RAM | Query Latency |
|:---:|:---:|:---:|
| 10K | 30 MB | 50ms |
| 100K | 295 MB | 80ms |
| 1M | 2.8 GB | 150ms |

---

## üìö Documenta√ß√£o

A documenta√ß√£o completa est√° organizada em `docs/`:

```
docs/
‚îú‚îÄ‚îÄ SYSTEM_OVERVIEW.md    # Vis√£o geral da arquitetura
‚îú‚îÄ‚îÄ modules/               # Documenta√ß√£o por m√≥dulo
‚îú‚îÄ‚îÄ reports/               # Relat√≥rios de an√°lise
‚îî‚îÄ‚îÄ tutorials/             # Tutoriais passo-a-passo
```

**Relat√≥rios T√©cnicos** (gerados automaticamente):
- VQ-VAE Deep Analysis
- Experimental Ablation Suite (A-D)
- Module Integration Status
- Cleanup Reports

---

## ÔøΩ Quick Start

```bash
# Clone
git clone https://github.com/GAndreuu/Prototype-Alexandria.git
cd Alexandria

# Setup
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt

# Initialize
python scripts/init_brain.py

# Index documents
from core.memory.semantic_memory import SemanticFileSystem
from core.topology.topology_engine import TopologyEngine

engine = TopologyEngine()
memory = SemanticFileSystem(engine)
memory.index_file("path/to/document.pdf")

# Query
results = memory.retrieve("quantum computing", limit=10)
```

**Interface**:
```bash
streamlit run interface/app.py
# Access: http://localhost:8501
```

---

## ÔøΩÔ∏è Roadmap

### ‚úÖ Fase Atual: Production Ready
- [x] VQ-VAE compression & balance
- [x] Mycelial reasoning network
- [x] Nemesis Core integration
- [x] Complete system verification

### üîÑ Pr√≥xima Fase: Enhanced Intelligence
- [ ] Chain-of-Thought integration
- [ ] Local LLM integration (Llama 3)
- [ ] Tool use framework
- [ ] Self-reflection loops

### üåü Fase Futura: Meta-Learning
- [ ] Performance tracking dashboard
- [ ] Automated hyperparameter search
- [ ] Curriculum generation
- [ ] Multi-task learning

---

## ü§ù Contribuindo

Alexandria √© open-source e aceita contribui√ß√µes! Veja [CONTRIBUTING.md](CONTRIBUTING.md).

---

## ÔøΩ Licen√ßa

MIT License - Veja [LICENSE](LICENSE) para detalhes.

---

<div align="center">

**Alexandria Cognitive System**

*Building local AGI, one commit at a time*

‚≠ê Star se Alexandria ajudou sua pesquisa!

**Contato**: [gabrielandreu82@hotmail.com](mailto:gabrielandreu82@hotmail.com)

</div>
