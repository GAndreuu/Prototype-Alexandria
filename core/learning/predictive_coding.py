"""
Predictive Coding Module for Alexandria
========================================

Implementa√ß√£o de Predictive Coding hier√°rquico para a rede Alexandria.
Baseado em: Karl Friston's Free Energy Principle e Predictive Processing.

Este m√≥dulo implementa a evolu√ß√£o natural do Meta-Hebbian:
- Em vez de propagar ATIVA√á√ïES, propaga ERROS de predi√ß√£o
- Cada camada prediz o que vai receber e aprende com a diferen√ßa
- Mais eficiente (s√≥ transmite surpresa/novidade)
- Biologicamente plaus√≠vel
- Prepara√ß√£o para Active Inference

Hierarquia de paradigmas:
    Hebbian ‚Üí Meta-Hebbian ‚Üí Predictive Coding ‚Üí Active Inference ‚Üí Free Energy
                                    ‚Üë
                               VOC√ä EST√Å AQUI

Refer√™ncias:
- Rao & Ballard (1999) - Predictive coding in visual cortex
- Friston (2005) - A theory of cortical responses
- Whittington & Bogacz (2017) - Approximation of backprop

Autor: G (Alexandria Project)
Vers√£o: 1.0
"""

import numpy as np
from typing import Dict, Any, Optional, Tuple, List, Callable
from dataclasses import dataclass, field
from enum import Enum
import pickle
from pathlib import Path

# Import do Meta-Hebbian (assumindo que est√° no mesmo diret√≥rio)
try:
    from meta_hebbian import MetaHebbianPlasticity, PlasticityRule, create_meta_hebbian_system
except ImportError:
    # Fallback se rodando standalone
    MetaHebbianPlasticity = None


# =============================================================================
# CONFIGURA√á√ÉO E TIPOS
# =============================================================================

class PrecisionMode(Enum):
    """Como a precis√£o (confian√ßa) √© computada"""
    FIXED = "fixed"              # Precis√£o fixa
    LEARNED = "learned"          # Precis√£o aprendida por camada
    ADAPTIVE = "adaptive"        # Precis√£o adapta em runtime


@dataclass
class PredictiveCodingConfig:
    """Configura√ß√£o do sistema de Predictive Coding"""
    
    # Arquitetura
    input_dim: int = 384                    # Dimens√£o do embedding (all-MiniLM)
    hidden_dims: List[int] = field(default_factory=lambda: [256, 128, 64])
    code_dim: int = 32                      # Dimens√£o do c√≥digo latente
    
    # Din√¢mica
    num_iterations: int = 10                # Itera√ß√µes de infer√™ncia por input
    inference_lr: float = 0.1               # Learning rate da infer√™ncia
    learning_lr: float = 0.01               # Learning rate do aprendizado
    
    # Precis√£o (inverse variance)
    precision_mode: PrecisionMode = PrecisionMode.ADAPTIVE
    base_precision: float = 1.0             # Precis√£o base
    precision_lr: float = 0.001             # LR para aprender precis√£o
    
    # Integra√ß√£o
    use_meta_hebbian: bool = True           # Usar Meta-Hebbian para pesos
    prediction_noise: float = 0.01          # Ru√≠do nas predi√ß√µes (regulariza√ß√£o)
    
    # Persist√™ncia
    save_path: str = "data/predictive_coding_state.pkl"


# =============================================================================
# CAMADA PREDITIVA
# =============================================================================

class PredictiveLayer:
    """
    Uma camada no modelo de Predictive Coding.
    
    Cada camada:
    1. Recebe predi√ß√£o top-down da camada acima
    2. Recebe input bottom-up da camada abaixo
    3. Computa erro de predi√ß√£o
    4. Atualiza representa√ß√£o para minimizar erro
    5. Propaga erro (n√£o ativa√ß√£o!) para cima e para baixo
    
    Equa√ß√µes principais:
        prediction_error = input - prediction
        representation += lr * (precision * prediction_error - lateral_inhibition)
    """
    
    def __init__(
        self,
        input_dim: int,
        output_dim: int,
        layer_id: int,
        config: PredictiveCodingConfig
    ):
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.layer_id = layer_id
        self.config = config
        
        # Pesos
        self._init_weights()
        
        # Estado
        self.representation = np.zeros(output_dim)      # Œº (mean)
        self.prediction_error = np.zeros(input_dim)     # Œµ (error)
        self.precision = np.ones(input_dim) * config.base_precision  # Œ† (precision)
        
        # Hist√≥rico
        self.error_history: List[float] = []
        self.precision_history: List[float] = []
        
    def _init_weights(self):
        """Inicializa pesos com Xavier/Glorot"""
        scale = np.sqrt(2.0 / (self.input_dim + self.output_dim))
        
        # W_pred: gera predi√ß√£o para camada abaixo
        self.W_pred = np.random.randn(self.input_dim, self.output_dim) * scale
        
        # W_err: processa erro da camada abaixo
        self.W_err = np.random.randn(self.output_dim, self.input_dim) * scale
        
        # Bias
        self.b_pred = np.zeros(self.input_dim)
        self.b_rep = np.zeros(self.output_dim)
        
    def predict(self, top_down_input: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Gera predi√ß√£o para a camada abaixo.
        
        prediction = W_pred @ representation + bias + noise
        """
        prediction = self.W_pred @ self.representation + self.b_pred
        
        # Adiciona ru√≠do para regulariza√ß√£o
        if self.config.prediction_noise > 0:
            prediction += np.random.randn(*prediction.shape) * self.config.prediction_noise
        
        # Non-linearity (ReLU suave)
        prediction = self._softplus(prediction)
        
        return prediction
    
    def compute_error(self, bottom_up_input: np.ndarray) -> np.ndarray:
        """
        Computa erro de predi√ß√£o.
        
        error = precision * (input - prediction)
        
        Precision-weighted: erros em dimens√µes "confi√°veis" pesam mais.
        """
        prediction = self.predict()
        raw_error = bottom_up_input - prediction
        
        # Precision weighting
        self.prediction_error = self.precision * raw_error
        
        # Registra magnitude do erro
        self.error_history.append(float(np.mean(np.abs(self.prediction_error))))
        
        return self.prediction_error
    
    def update_representation(
        self,
        bottom_up_error: np.ndarray,
        top_down_prediction: Optional[np.ndarray] = None,
        iterations: Optional[int] = None
    ) -> np.ndarray:
        """
        Atualiza representa√ß√£o interna para minimizar erro.
        
        Isso √© a "infer√™ncia" no Predictive Coding:
        - N√£o √© feedforward pass
        - √â um processo iterativo de settling
        - Minimiza energia livre variacional
        
        ŒîŒº = lr * (W_err @ Œµ_below - Œµ_self)
        """
        iterations = iterations or self.config.num_iterations
        
        for _ in range(iterations):
            # Gradiente do erro bottom-up
            # W_err: (output_dim, input_dim), bottom_up_error: (input_dim,)
            # Resultado: (output_dim,) - mesmo tamanho que representation
            gradient_bottom = self.W_err @ bottom_up_error
            
            # Gradiente do erro top-down (se houver)
            if top_down_prediction is not None and len(top_down_prediction) == len(self.representation):
                error_top = self.representation - top_down_prediction
                gradient_top = -error_top
            else:
                gradient_top = np.zeros_like(self.representation)
            
            # Atualiza representa√ß√£o
            total_gradient = gradient_bottom + gradient_top
            self.representation += self.config.inference_lr * total_gradient
            
            # Regulariza√ß√£o (mant√©m representa√ß√£o bounded)
            self.representation = np.clip(self.representation, -10, 10)
        
        return self.representation
    
    def learn(self, bottom_up_input: np.ndarray):
        """
        Atualiza pesos para melhorar predi√ß√µes futuras.
        
        ŒîW_pred = lr * Œµ @ Œº.T  (Hebbian no erro!)
        """
        # Computa erro atual
        error = self.compute_error(bottom_up_input)
        
        # Gradiente para W_pred
        # Minimiza ||input - W_pred @ representation||¬≤
        dW_pred = np.outer(error, self.representation) * self.config.learning_lr
        self.W_pred += dW_pred
        
        # Gradiente para bias
        db_pred = error * self.config.learning_lr
        self.b_pred += db_pred
        
        # Atualiza precis√£o se modo adaptativo
        if self.config.precision_mode == PrecisionMode.ADAPTIVE:
            self._update_precision(error)
        
        return {
            'error_magnitude': float(np.mean(np.abs(error))),
            'weight_change': float(np.mean(np.abs(dW_pred))),
            'precision_mean': float(np.mean(self.precision))
        }
    
    def _update_precision(self, error: np.ndarray):
        """
        Atualiza precis√£o baseado na vari√¢ncia do erro.
        
        Precis√£o alta = erro consistentemente baixo = confian√ßa alta
        Precis√£o baixa = erro vari√°vel = confian√ßa baixa
        """
        # Erro quadr√°tico como proxy para vari√¢ncia
        error_variance = error ** 2
        
        # Precis√£o √© inverso da vari√¢ncia (com suaviza√ß√£o)
        target_precision = 1.0 / (error_variance + 0.01)
        
        # Atualiza√ß√£o suave
        self.precision += self.config.precision_lr * (target_precision - self.precision)
        
        # Clamp para estabilidade
        self.precision = np.clip(self.precision, 0.1, 10.0)
        
        self.precision_history.append(float(np.mean(self.precision)))
    
    def _softplus(self, x: np.ndarray) -> np.ndarray:
        """Softplus activation: log(1 + exp(x))"""
        return np.log1p(np.exp(np.clip(x, -20, 20)))
    
    def get_state(self) -> Dict[str, Any]:
        """Retorna estado completo da camada"""
        return {
            'representation': self.representation.copy(),
            'prediction_error': self.prediction_error.copy(),
            'precision': self.precision.copy(),
            'W_pred': self.W_pred.copy(),
            'W_err': self.W_err.copy(),
            'b_pred': self.b_pred.copy(),
            'error_history': self.error_history[-100:],
            'precision_history': self.precision_history[-100:]
        }
    
    def set_state(self, state: Dict[str, Any]):
        """Restaura estado da camada"""
        self.representation = state['representation']
        self.prediction_error = state['prediction_error']
        self.precision = state['precision']
        self.W_pred = state['W_pred']
        self.W_err = state['W_err']
        self.b_pred = state['b_pred']
        self.error_history = state.get('error_history', [])
        self.precision_history = state.get('precision_history', [])


# =============================================================================
# REDE DE PREDICTIVE CODING
# =============================================================================

class PredictiveCodingNetwork:
    """
    Rede hier√°rquica de Predictive Coding.
    
    Arquitetura:
        Input (384D) ‚Üí Layer 1 (256D) ‚Üí Layer 2 (128D) ‚Üí Layer 3 (64D) ‚Üí Code (32D)
        
    Fluxo de informa√ß√£o:
        Bottom-up: erros de predi√ß√£o sobem
        Top-down: predi√ß√µes descem
        
    Diferen√ßa do feedforward tradicional:
        - N√£o √© um pass √∫nico
        - √â um processo iterativo de settling
        - Converge para estado de m√≠nima energia livre
    """
    
    def __init__(self, config: Optional[PredictiveCodingConfig] = None):
        self.config = config or PredictiveCodingConfig()
        
        # Constr√≥i camadas
        self.layers: List[PredictiveLayer] = []
        self._build_network()
        
        # Meta-Hebbian opcional
        self.meta_hebbian = None
        if self.config.use_meta_hebbian and MetaHebbianPlasticity:
            self.meta_hebbian = create_meta_hebbian_system(
                num_codes=self.config.code_dim,
                num_heads=4,
                load_existing=False
            )
        
        # Estat√≠sticas
        self.total_observations = 0
        self.convergence_history: List[int] = []
        
    def _build_network(self):
        """Constr√≥i stack de camadas preditivas"""
        dims = [self.config.input_dim] + self.config.hidden_dims + [self.config.code_dim]
        
        for i in range(len(dims) - 1):
            layer = PredictiveLayer(
                input_dim=dims[i],
                output_dim=dims[i + 1],
                layer_id=i,
                config=self.config
            )
            self.layers.append(layer)
        
        print(f"üß† Predictive Coding Network constru√≠da:")
        print(f"   Camadas: {' ‚Üí '.join(str(d) for d in dims)}")
    
    def infer(
        self,
        input_data: np.ndarray,
        max_iterations: int = 50,
        convergence_threshold: float = 0.001
    ) -> Tuple[np.ndarray, Dict[str, Any]]:
        """
        Infer√™ncia: encontra representa√ß√£o latente que melhor explica o input.
        
        Processo iterativo:
        1. Propaga input bottom-up
        2. Gera predi√ß√µes top-down
        3. Computa erros
        4. Atualiza representa√ß√µes
        5. Repete at√© convergir
        
        Returns:
            code: Representa√ß√£o latente final
            stats: Estat√≠sticas da infer√™ncia
        """
        # Inicializa representa√ß√µes
        for layer in self.layers:
            layer.representation = np.random.randn(layer.output_dim) * 0.1
        
        prev_total_error = float('inf')
        errors_over_time = []
        
        for iteration in range(max_iterations):
            total_error = 0
            
            # Bottom-up pass: computa erros
            current_input = input_data
            for layer in self.layers:
                error = layer.compute_error(current_input)
                total_error += np.mean(np.abs(error))
                current_input = layer.representation
            
            # Top-down pass: atualiza representa√ß√µes
            top_down_pred = None
            for layer in reversed(self.layers):
                # Cada camada usa seu pr√≥prio erro de predi√ß√£o
                bottom_up_err = layer.prediction_error
                
                layer.update_representation(bottom_up_err, top_down_pred, iterations=1)
                
                # Predi√ß√£o para a pr√≥xima camada (acima)
                if layer.layer_id > 0:
                    top_down_pred = layer.representation  # Passa representa√ß√£o como target
            
            errors_over_time.append(total_error)
            
            # Verifica converg√™ncia
            error_change = abs(prev_total_error - total_error)
            if error_change < convergence_threshold:
                self.convergence_history.append(iteration)
                break
            
            prev_total_error = total_error
        
        # C√≥digo latente √© a representa√ß√£o da √∫ltima camada
        code = self.layers[-1].representation.copy()
        
        self.total_observations += 1
        
        return code, {
            'iterations': iteration + 1,
            'final_error': total_error,
            'converged': error_change < convergence_threshold,
            'errors_over_time': errors_over_time
        }
    
    def learn_from_input(self, input_data: np.ndarray) -> Dict[str, Any]:
        """
        Aprendizado: atualiza pesos para melhorar predi√ß√µes.
        
        1. Primeiro faz infer√™ncia (encontra melhor representa√ß√£o)
        2. Depois atualiza pesos baseado nos erros
        """
        # Infer√™ncia
        code, infer_stats = self.infer(input_data)
        
        # Aprendizado camada por camada
        learn_stats = []
        current_input = input_data
        
        for layer in self.layers:
            stats = layer.learn(current_input)
            learn_stats.append(stats)
            current_input = layer.representation
        
        # Se usando Meta-Hebbian, evolui regras periodicamente
        meta_stats = None
        if self.meta_hebbian and self.total_observations % 50 == 0:
            # Usa erro como proxy para fitness (menor erro = maior fitness)
            fitness = 1.0 / (infer_stats['final_error'] + 0.01)
            meta_stats = self.meta_hebbian.evolve_rules([fitness])
        
        return {
            'inference': infer_stats,
            'learning': learn_stats,
            'meta_hebbian': meta_stats,
            'code': code
        }
    
    def encode(self, input_data: np.ndarray) -> np.ndarray:
        """Encoding r√°pido (infer√™ncia completa)"""
        code, _ = self.infer(input_data)
        return code
    
    def decode(self, code: np.ndarray) -> np.ndarray:
        """
        Decoding: gera predi√ß√£o do input a partir do c√≥digo.
        
        Propaga top-down atrav√©s das camadas.
        Cada camada gera predi√ß√£o que serve como representa√ß√£o da camada abaixo.
        """
        # Define representa√ß√£o da √∫ltima camada
        self.layers[-1].representation = code.copy()
        
        # Propaga top-down
        for i in range(len(self.layers) - 1, 0, -1):
            # Camada i gera predi√ß√£o
            prediction = self.layers[i].predict()
            # Predi√ß√£o tem dimens√£o input_dim[i] = output_dim[i-1]
            # Ent√£o pode ser usada diretamente como representa√ß√£o de i-1
            self.layers[i - 1].representation = prediction
        
        # Predi√ß√£o final √© a predi√ß√£o da primeira camada (384D)
        return self.layers[0].predict()
    
    def get_prediction_errors(self) -> List[np.ndarray]:
        """Retorna erros de predi√ß√£o de todas as camadas"""
        return [layer.prediction_error.copy() for layer in self.layers]
    
    def get_precisions(self) -> List[np.ndarray]:
        """Retorna precis√µes de todas as camadas"""
        return [layer.precision.copy() for layer in self.layers]
    
    # =========================================================================
    # AN√ÅLISE E DIAGN√ìSTICO
    # =========================================================================
    
    def get_network_analysis(self) -> Dict[str, Any]:
        """An√°lise completa da rede"""
        layer_stats = []
        for i, layer in enumerate(self.layers):
            layer_stats.append({
                'layer_id': i,
                'dims': f"{layer.input_dim} ‚Üí {layer.output_dim}",
                'mean_error': np.mean(layer.error_history[-10:]) if layer.error_history else 0,
                'mean_precision': np.mean(layer.precision),
                'precision_std': np.std(layer.precision),
                'representation_norm': np.linalg.norm(layer.representation),
                'weight_norm': np.linalg.norm(layer.W_pred)
            })
        
        # Converg√™ncia
        avg_iterations = np.mean(self.convergence_history[-100:]) if self.convergence_history else 0
        
        return {
            'layers': layer_stats,
            'total_observations': self.total_observations,
            'avg_convergence_iterations': avg_iterations,
            'meta_hebbian_active': self.meta_hebbian is not None,
            'interpretation': self._interpret_state()
        }
    
    def _interpret_state(self) -> str:
        """Interpreta√ß√£o leg√≠vel do estado"""
        if not self.convergence_history:
            return "INITIALIZING: Ainda sem observa√ß√µes"
        
        avg_iterations = np.mean(self.convergence_history[-20:])
        avg_error = np.mean([
            np.mean(l.error_history[-10:]) if l.error_history else 1.0 
            for l in self.layers
        ])
        
        if avg_iterations < 5 and avg_error < 0.1:
            return "EFFICIENT: Converg√™ncia r√°pida, erros baixos"
        elif avg_iterations < 10:
            return "LEARNING: Converg√™ncia boa, ainda otimizando"
        elif avg_error > 0.5:
            return "STRUGGLING: Erros altos, modelo precisa mais treino"
        else:
            return "EXPLORING: Estado intermedi√°rio de aprendizado"
    
    # =========================================================================
    # PERSIST√äNCIA
    # =========================================================================
    
    def save_state(self, path: Optional[str] = None):
        """Salva estado completo da rede"""
        path = path or self.config.save_path
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        
        state = {
            'config': {
                'input_dim': self.config.input_dim,
                'hidden_dims': self.config.hidden_dims,
                'code_dim': self.config.code_dim,
                'num_iterations': self.config.num_iterations,
                'precision_mode': self.config.precision_mode.value
            },
            'layers': [layer.get_state() for layer in self.layers],
            'total_observations': self.total_observations,
            'convergence_history': self.convergence_history[-1000:]
        }
        
        with open(path, 'wb') as f:
            pickle.dump(state, f)
        
        # Salva Meta-Hebbian separadamente se existir
        if self.meta_hebbian:
            self.meta_hebbian.save_state()
        
        return path
    
    def load_state(self, path: Optional[str] = None) -> bool:
        """Carrega estado salvo"""
        path = path or self.config.save_path
        
        if not Path(path).exists():
            return False
        
        try:
            with open(path, 'rb') as f:
                state = pickle.load(f)
            
            for i, layer_state in enumerate(state['layers']):
                if i < len(self.layers):
                    self.layers[i].set_state(layer_state)
            
            self.total_observations = state.get('total_observations', 0)
            self.convergence_history = state.get('convergence_history', [])
            
            # Carrega Meta-Hebbian
            if self.meta_hebbian:
                self.meta_hebbian.load_state()
            
            return True
        except Exception as e:
            print(f"Erro ao carregar Predictive Coding: {e}")
            return False


# =============================================================================
# INTEGRA√á√ÉO COM ALEXANDRIA
# =============================================================================

class PredictiveCodingAlexandriaIntegration:
    """
    Integra√ß√£o do Predictive Coding com o sistema Alexandria.
    
    Substitui/complementa o pipeline:
        Embedding ‚Üí VQ-VAE ‚Üí Mycelial
    
    Por:
        Embedding ‚Üí PredictiveCoding ‚Üí VQ-VAE ‚Üí Mycelial
        
    Benef√≠cios:
    - Representa√ß√µes mais compactas (s√≥ novidade)
    - Melhor generaliza√ß√£o
    - Prepara√ß√£o para Active Inference
    """
    
    def __init__(
        self,
        pc_network: PredictiveCodingNetwork,
        vqvae_encoder: Optional[Any] = None,
        mycelial: Optional[Any] = None
    ):
        self.pc = pc_network
        self.vqvae = vqvae_encoder
        self.mycelial = mycelial
        
        # Buffer para batch processing
        self.embedding_buffer: List[np.ndarray] = []
        self.buffer_size = 32
        
    def process_embedding(
        self,
        embedding: np.ndarray,
        learn: bool = True
    ) -> Dict[str, Any]:
        """
        Processa um embedding atrav√©s do pipeline PC.
        
        Args:
            embedding: Vetor 384D do sentence-transformer
            learn: Se True, atualiza pesos
            
        Returns:
            result: Dicion√°rio com c√≥digo, erros, etc.
        """
        if learn:
            result = self.pc.learn_from_input(embedding)
        else:
            code, infer_stats = self.pc.infer(embedding)
            result = {'code': code, 'inference': infer_stats}
        
        # Se tiver VQ-VAE, quantiza o c√≥digo PC
        if self.vqvae is not None:
            pc_code = result['code']
            # Expande c√≥digo PC para dimens√£o esperada pelo VQ-VAE se necess√°rio
            if hasattr(self.vqvae, 'encode'):
                vq_indices = self.vqvae.encode(pc_code)
                result['vq_indices'] = vq_indices
        
        # Se tiver Mycelial, observa
        if self.mycelial is not None and 'vq_indices' in result:
            self.mycelial.observe(result['vq_indices'])
        
        return result
    
    def process_batch(
        self,
        embeddings: List[np.ndarray],
        learn: bool = True
    ) -> Dict[str, Any]:
        """Processa batch de embeddings"""
        results = []
        total_error = 0
        
        for emb in embeddings:
            result = self.process_embedding(emb, learn=learn)
            results.append(result)
            if 'inference' in result:
                total_error += result['inference'].get('final_error', 0)
        
        return {
            'batch_size': len(embeddings),
            'mean_error': total_error / len(embeddings) if embeddings else 0,
            'results': results
        }
    
    def get_surprise_signal(self, embedding: np.ndarray) -> float:
        """
        Computa "surpresa" do input.
        
        Surpresa alta = input muito diferente do esperado
        Surpresa baixa = input previs√≠vel
        
        Isso √© √∫til para:
        - Detectar outliers
        - Priorizar aprendizado de coisas novas
        - Active Inference (pr√≥ximo passo)
        """
        _, stats = self.pc.infer(embedding, max_iterations=5)
        
        # Surpresa √© proporcional ao erro de predi√ß√£o
        surprise = stats['final_error']
        
        return surprise
    
    def get_integration_stats(self) -> Dict[str, Any]:
        """Estat√≠sticas da integra√ß√£o"""
        return {
            'pc_stats': self.pc.get_network_analysis(),
            'has_vqvae': self.vqvae is not None,
            'has_mycelial': self.mycelial is not None,
            'buffer_size': len(self.embedding_buffer)
        }


# =============================================================================
# ACTIVE INFERENCE PREVIEW (pr√≥ximo passo)
# =============================================================================

class ActiveInferencePreview:
    """
    Preview de Active Inference.
    
    Active Inference = Predictive Coding + A√á√ÉO
    
    O sistema n√£o s√≥ prediz passivamente, mas ATUA no mundo
    para confirmar suas predi√ß√µes (ou reduzir incerteza).
    
    Para Alexandria, isso significaria:
    - Sistema que busca ativamente papers para preencher gaps
    - Queries que o sistema gera sozinho
    - Explora√ß√£o aut√¥noma do espa√ßo de conhecimento
    
    NOTA: Esta √© uma preview. Implementa√ß√£o completa requer
    definir espa√ßo de a√ß√µes (queries, navega√ß√£o, etc.)
    """
    
    def __init__(self, pc_network: PredictiveCodingNetwork):
        self.pc = pc_network
        self.action_history: List[Dict] = []
        
    def compute_expected_free_energy(
        self,
        possible_actions: List[str],
        current_state: np.ndarray
    ) -> List[Tuple[str, float]]:
        """
        Computa energia livre esperada para cada a√ß√£o poss√≠vel.
        
        G = E[log P(o|œÄ) - log Q(o|œÄ)] + E[H(o|s,œÄ)]
        
        Simplificado:
        G ‚âà uncertainty_reduction - information_gain
        
        A√ß√£o √≥tima = minimiza G
        """
        action_scores = []
        
        for action in possible_actions:
            # Simula efeito da a√ß√£o (simplificado)
            # Em implementa√ß√£o real, usaria modelo do mundo
            
            # Proxy: a√ß√µes que reduzem incerteza nas camadas
            uncertainty = np.mean([np.mean(l.precision) for l in self.pc.layers])
            
            # Score: menor G √© melhor
            G = -uncertainty  # Simplifica√ß√£o
            
            action_scores.append((action, G))
        
        # Ordena por G (menor √© melhor)
        action_scores.sort(key=lambda x: x[1])
        
        return action_scores
    
    def suggest_next_action(self) -> Dict[str, Any]:
        """
        Sugere pr√≥xima a√ß√£o baseado em Active Inference.
        
        Para Alexandria:
        - Se incerteza alta em cluster X ‚Üí buscar papers sobre X
        - Se conex√£o fraca entre A e B ‚Üí buscar papers que conectam
        """
        # Analisa estado atual
        analysis = self.pc.get_network_analysis()
        
        # Identifica √°reas de alta incerteza (baixa precis√£o)
        low_precision_layers = [
            l for l in analysis['layers'] 
            if l['mean_precision'] < 1.0
        ]
        
        if low_precision_layers:
            return {
                'action_type': 'EXPLORE',
                'target': f"Layer {low_precision_layers[0]['layer_id']}",
                'reason': 'Alta incerteza detectada',
                'priority': 1.0 - low_precision_layers[0]['mean_precision']
            }
        else:
            return {
                'action_type': 'CONSOLIDATE',
                'target': 'All layers',
                'reason': 'Sistema est√°vel, consolidar conhecimento',
                'priority': 0.3
            }


# =============================================================================
# FUN√á√ïES DE CONVENI√äNCIA
# =============================================================================

def create_predictive_coding_system(
    input_dim: int = 384,
    hidden_dims: Optional[List[int]] = None,
    code_dim: int = 32,
    load_existing: bool = True,
    use_meta_hebbian: bool = True
) -> PredictiveCodingNetwork:
    """
    Factory function para criar sistema de Predictive Coding.
    """
    config = PredictiveCodingConfig(
        input_dim=input_dim,
        hidden_dims=hidden_dims or [256, 128, 64],
        code_dim=code_dim,
        use_meta_hebbian=use_meta_hebbian
    )
    
    network = PredictiveCodingNetwork(config)
    
    if load_existing:
        loaded = network.load_state()
        if loaded:
            print(f"‚úÖ Predictive Coding carregado: {network.total_observations} observa√ß√µes")
        else:
            print("üå± Predictive Coding inicializado fresh")
    
    return network


def integrate_with_alexandria(
    pc_network: PredictiveCodingNetwork,
    vqvae=None,
    mycelial=None
) -> PredictiveCodingAlexandriaIntegration:
    """
    Integra Predictive Coding com componentes Alexandria.
    """
    return PredictiveCodingAlexandriaIntegration(pc_network, vqvae, mycelial)


# =============================================================================
# EXEMPLO DE USO E TESTES
# =============================================================================

if __name__ == "__main__":
    print("=" * 70)
    print("PREDICTIVE CODING - ALEXANDRIA")
    print("=" * 70)
    
    # Criar rede
    pc = create_predictive_coding_system(
        input_dim=384,
        hidden_dims=[256, 128, 64],
        code_dim=32,
        use_meta_hebbian=True
    )
    
    # Simular alguns inputs
    print("\nüîÑ SIMULANDO APRENDIZADO...")
    
    for i in range(20):
        # Embedding fake (normalmente viria do sentence-transformer)
        fake_embedding = np.random.randn(384)
        fake_embedding = fake_embedding / np.linalg.norm(fake_embedding)
        
        # Aprende
        result = pc.learn_from_input(fake_embedding)
        
        if i % 5 == 0:
            print(f"   Obs {i+1}: erro={result['inference']['final_error']:.4f}, "
                  f"iters={result['inference']['iterations']}")
    
    # An√°lise
    print("\nüìä AN√ÅLISE DA REDE:")
    analysis = pc.get_network_analysis()
    
    for layer in analysis['layers']:
        print(f"\n   Layer {layer['layer_id']} ({layer['dims']}):")
        print(f"      Erro m√©dio: {layer['mean_error']:.4f}")
        print(f"      Precis√£o: {layer['mean_precision']:.4f} ¬± {layer['precision_std']:.4f}")
    
    print(f"\nüéØ ESTADO: {analysis['interpretation']}")
    print(f"   Observa√ß√µes: {analysis['total_observations']}")
    print(f"   Converg√™ncia m√©dia: {analysis['avg_convergence_iterations']:.1f} itera√ß√µes")
    
    # Teste de encoding/decoding
    print("\nüîÑ TESTE DE RECONSTRUCTION:")
    test_input = np.random.randn(384)
    test_input = test_input / np.linalg.norm(test_input)
    
    code = pc.encode(test_input)
    reconstruction = pc.decode(code)
    
    recon_error = np.mean((test_input - reconstruction) ** 2)
    print(f"   Erro de reconstru√ß√£o: {recon_error:.4f}")
    print(f"   Compress√£o: 384D ‚Üí {len(code)}D ({len(code)/384*100:.1f}%)")
    
    # Active Inference preview
    print("\nüîÆ ACTIVE INFERENCE PREVIEW:")
    ai_preview = ActiveInferencePreview(pc)
    suggestion = ai_preview.suggest_next_action()
    print(f"   A√ß√£o sugerida: {suggestion['action_type']}")
    print(f"   Target: {suggestion['target']}")
    print(f"   Raz√£o: {suggestion['reason']}")
    
    # Salvar
    save_path = pc.save_state()
    print(f"\nüíæ Estado salvo em: {save_path}")
    
    print("\n" + "=" * 70)
    print("‚úÖ PREDICTIVE CODING PRONTO PARA INTEGRA√á√ÉO")
    print("=" * 70)
    
    print("""
    
ARQUITETURA COMPLETA:
=====================

    Input (embedding 384D)
           ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ     PREDICTIVE CODING NETWORK        ‚îÇ
    ‚îÇ                                      ‚îÇ
    ‚îÇ   Layer 1: 384 ‚Üí 256                ‚îÇ
    ‚îÇ      ‚Üì erro ‚Üë predi√ß√£o              ‚îÇ
    ‚îÇ   Layer 2: 256 ‚Üí 128                ‚îÇ
    ‚îÇ      ‚Üì erro ‚Üë predi√ß√£o              ‚îÇ
    ‚îÇ   Layer 3: 128 ‚Üí 64                 ‚îÇ
    ‚îÇ      ‚Üì erro ‚Üë predi√ß√£o              ‚îÇ
    ‚îÇ   Layer 4: 64 ‚Üí 32                  ‚îÇ
    ‚îÇ                                      ‚îÇ
    ‚îÇ   + Meta-Hebbian (regras adaptivas)  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚Üì
    Code (32D) ‚Üí VQ-VAE ‚Üí Mycelial
    

CAMINHO DE EVOLU√á√ÉO:
====================

    ‚úÖ Hebbian (base)
    ‚úÖ Meta-Hebbian (regras aprendidas)
    ‚úÖ Predictive Coding (este arquivo)
    ‚¨ú Active Inference (preview inclu√≠do)
    ‚¨ú Free Energy completo (futuro)
    

INTEGRA√á√ÉO:
===========

    from predictive_coding import create_predictive_coding_system, integrate_with_alexandria
    
    # Criar sistema
    pc = create_predictive_coding_system()
    
    # Integrar com Alexandria
    integration = integrate_with_alexandria(pc, vqvae, mycelial)
    
    # Processar embeddings
    for embedding in embeddings:
        result = integration.process_embedding(embedding)
        
    # Checar surpresa
    surprise = integration.get_surprise_signal(new_embedding)
    if surprise > threshold:
        print("Input muito novo/surpreendente!")
    
    """)
