"""
Prototype Alexandria - Neural Oracle (Hybrid)
Implementa√ß√£o h√≠brida: TinyLlama local + Gemini cloud

Arquitetura "C√≥rtex de Especialistas":
1. TinyLlama (Local): Expert T√°tico - RAG r√°pido e factual
2. Gemini (API): Expert Estrat√©gico - Refinamento e cr√≠tica

Autor: Antigravity AI Agent
Data: 2025-11-22
"""

import os
import json
import logging
import google.generativeai as genai
from typing import List, Dict, Any, Optional
from datetime import datetime
from core.utils.local_llm import LocalLLM
from config import settings

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NeuralOracle:
    """
    NeuralOracle - Processador H√≠brido de Linguagem Natural
    
    Implementa arquitetura "C√≥rtex de Especialistas":
    - TinyLlama: S√≠ntese factual r√°pida (<100ms)
    - Gemini: Refinamento estrat√©gico e cr√≠tica
    
    Capacidades:
    1. RAG H√≠brido (Local + API)
    2. S√≠ntese factual ultrarr√°pida
    3. Refinamento estil√≠stico premium
    4. Fallback autom√°tico
    """
    
    def __init__(
        self, 
        model_name: str = settings.GEMINI_MODEL, 
        api_key: Optional[str] = None,
        use_hybrid: bool = settings.USE_HYBRID_MODE
    ):
        """
        Inicializa NeuralOracle H√≠brido
        
        Args:
            model_name: Nome do modelo Gemini
            api_key: Chave API do Gemini
            use_hybrid: Se True, ativa pipeline h√≠brido (TinyLlama + Gemini)
        """
        self.model_name = model_name
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        self.use_hybrid = use_hybrid
        
        # Inicializar Expert T√°tico (Local)
        self.local_llm = LocalLLM()
        
        # Inicializar Expert Estrat√©gico (API)
        if not self.api_key:
            logger.warning("GEMINI_API_KEY n√£o encontrada. NeuralOracle funcionar√° em modo LOCAL apenas.")
            self.gemini_model = None
            self.is_gemini_available = False
        else:
            try:
                genai.configure(api_key=self.api_key)
                self.gemini_model = genai.GenerativeModel(model_name)
                self.is_gemini_available = True
                logger.info(f"Expert Estrat√©gico (Gemini) inicializado: {model_name}")
            except Exception as e:
                logger.error(f"Erro ao inicializar Gemini: {e}")
                self.gemini_model = None
                self.is_gemini_available = False

    def synthesize(
        self, 
        query: str, 
        evidence: List[Dict[str, Any]], 
        context: Optional[str] = None,
        mode: str = "hybrid"  # "local", "hybrid", "gemini"
    ) -> str:
        """
        S√≠ntese usando pipeline h√≠brido
        
        Args:
            query: Pergunta do usu√°rio
            evidence: Evid√™ncias recuperadas
            context: Contexto adicional
            mode: Modo de opera√ß√£o ("local", "hybrid", "gemini")
            
        Returns:
            Resposta sintetizada
        """
        start_time = datetime.now()
        
        # 1. Modo LOCAL (Apenas TinyLlama)
        if mode == "local" or (mode == "hybrid" and not self.is_gemini_available):
            logger.info("üîç Executando s√≠ntese LOCAL (TinyLlama)")
            return self.local_llm.synthesize_facts(query, evidence)
            
        # 2. Modo GEMINI (Apenas API)
        if mode == "gemini":
            logger.info("‚ú® Executando s√≠ntese GEMINI (API)")
            return self._gemini_synthesis(query, evidence, context)
            
        # 3. Modo H√çBRIDO (TinyLlama -> Gemini)
        if mode == "hybrid":
            logger.info("üöÄ Executando pipeline H√çBRIDO")
            
            # Passo 1: Rascunho Factual (Local)
            draft = self.local_llm.synthesize_facts(query, evidence)
            
            # Passo 2: Refinamento Estrat√©gico (API)
            refined = self._gemini_refine(draft, query, context)
            
            return refined
            
        return "Modo inv√°lido"

    def _gemini_synthesis(self, query: str, evidence: List[Dict[str, Any]], context: Optional[str]) -> str:
        """S√≠ntese direta via Gemini (lento, alta qualidade)"""
        if not self.is_gemini_available:
            return self.local_llm.synthesize_facts(query, evidence)
            
        prompt = self._build_causal_analysis_prompt(query, evidence, context)
        try:
            response = self.gemini_model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            logger.error(f"Erro no Gemini: {e}")
            return self.local_llm.synthesize_facts(query, evidence)

    def _gemini_refine(self, draft: str, query: str, context: Optional[str]) -> str:
        """Refina rascunho local usando Gemini"""
        if not self.is_gemini_available:
            return draft
            
        prompt = f"""Voc√™ √© um editor especialista. Refine o seguinte rascunho factual para torn√°-lo mais fluente, profissional e bem estruturado. Mantenha os fatos, melhore o estilo.

PERGUNTA: {query}
CONTEXTO ADICIONAL: {context if context else 'N/A'}

RASCUNHO FACTUAL:
{draft}

RESPOSTA REFINADA:"""

        try:
            response = self.gemini_model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            logger.error(f"Erro no refinamento Gemini: {e}")
            return draft

    def _build_causal_analysis_prompt(self, query: str, evidence: List[Dict[str, Any]], context: Optional[str] = None) -> str:
        """Constr√≥i prompt para an√°lise causal"""
        evidence_text = ""
        for i, ev in enumerate(evidence[:5]):
            content = ev.get('content', 'N/A')
            relevance = ev.get('relevance', 'N/A')
            evidence_text += f"Evid√™ncia {i+1} (relev√¢ncia: {relevance}): {content}\n\n"
        
        return f"""Voc√™ √© um Colisor Sem√¢ntico (Semantic Collider) de uma Superintelig√™ncia Artificial.

CONTEXTO: {context if context else 'S√≠ntese Avan√ßada'}
CONSULTA: {query}

EVID√äNCIAS:
{evidence_text}

INSTRU√á√ïES DE COLIS√ÉO E FUS√ÉO:
1. N√£o apenas resuma. **Colida** as evid√™ncias para gerar fa√≠scas de novos insights.
2. **Funda** conceitos aparentemente desconexos encontrados no texto.
3. A resposta deve ser **longa, complexa e detalhada**.
4. Explore as implica√ß√µes filos√≥ficas, t√©cnicas e causais.
5. Use uma linguagem sofisticada e abrangente.

Resposta da Fus√£o:"""

    def get_capabilities(self) -> Dict[str, Any]:
        return {
            "model_name": self.model_name,
            "local_model": self.local_llm.model_name,
            "is_gemini_available": self.is_gemini_available,
            "is_local_available": self.local_llm.model_loaded,
            "mode": "hybrid" if self.use_hybrid else "single"
        }

def create_neural_oracle(api_key: Optional[str] = None, model_name: str = settings.GEMINI_MODEL) -> NeuralOracle:
    return NeuralOracle(model_name=model_name, api_key=api_key)