"""
Prototype Alexandria - Senior Critic Agent
Interactive Consciousness

This module implements the self-observation and intelligent criticism system,
using Gemini to evaluate hypotheses generated by the Abduction Engine.

Autor: Prototype Alexandria Team
Data: 2025-11-22
"""

import asyncio
import json
import logging
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import numpy as np
import requests

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RiskLevel(Enum):
    """N√≠veis de risco para hip√≥teses."""
    BAIXO = "baixo"
    MEDIO = "medio" 
    ALTO = "alto"
    CRITICO = "critico"

class TruthScore(Enum):
    """Categorias de pontua√ß√£o de veracidade."""
    FACTUAL = "factual"
    PLAUSIVEL = "plausivel"
    ESPECULATIVO = "especulativo"
    FALSO = "falso"

@dataclass
class CriticalAssessment:
    """Estrutura de avalia√ß√£o cr√≠tica."""
    hypothesis_id: str
    truth_score: float  # 0.0 a 1.0
    truth_category: TruthScore
    risk_level: RiskLevel
    confidence_intervals: Dict[str, float]
    evidence_quality: float  # Qualidade das evid√™ncias
    contradiction_strength: float  # For√ßa das contradi√ß√µes
    reasoning_coherence: float  # Coer√™ncia do racioc√≠nio
    supporting_facts: List[str]
    contradicting_facts: List[str]
    gaps_in_evidence: List[str]
    recommendation: str  # "aprovar", "revisar", "rejeitar"
    suggested_adjustments: List[str]
    assessed_at: datetime

@dataclass
class SystemFeedback:
    """Feedback do sistema para auto-regulagem."""
    timestamp: datetime
    total_assessments: int
    approval_rate: float
    average_truth_score: float
    common_contradictions: List[str]
    system_bias_indicators: Dict[str, float]
    recommended_temperature_adjustment: float
    recommended_variance_adjustment: float

class CriticAgent:
    """
    V10 - Agente Cr√≠tico S√™nior
    
    Responsabilidades:
    1. Avaliar hip√≥teses geradas pelo V9 usando Gemini
    2. Calcular Pontua√ß√µes de Risco baseadas em evid√™ncias
    3. Fornecer feedback para auto-regulagem do sistema
    4. Detectar padr√µes de bias e contradi√ß√µes
    5. Sugerir ajustes nos par√¢metros do TinyLlama e Manifold
    """
    
    def __init__(self, 
                 gemini_api_key: Optional[str] = None,
                 sfs_path: str = "./semantic_index",
                 temperature_history: List[float] = None,
                 risk_tolerance: float = 0.7):
        """
        Inicializa o Agente Cr√≠tico S√™nior.
        
        Args:
            gemini_api_key: Chave da API do Gemini
            sfs_path: Caminho para o Semantic File System
            temperature_history: Hist√≥rico de temperaturas para auto-regulagem
            risk_tolerance: Toler√¢ncia de risco do sistema (0.0 a 1.0)
        """
        # Carregar chave do ambiente diretamente
        self.gemini_api_key = gemini_api_key or os.getenv("GEMINI_API_KEY")
        self.sfs_path = sfs_path
        self.temperature_history = temperature_history or []
        self.risk_tolerance = risk_tolerance
        
        # Debug: Log status da chave
        logger.info(f"üîë Gemini API Key inicializada: {'‚úÖ Configurada' if self.gemini_api_key else '‚ùå N√£o configurada'}")
        if self.gemini_api_key:
            logger.info(f"üîë Primeiros caracteres da chave: {self.gemini_api_key[:4]}...")
        
        # Hist√≥rico de avalia√ß√µes
        self.assessment_history: List[CriticalAssessment] = []
        self.bias_indicators: Dict[str, float] = {
            "confirmation_bias": 0.0,
            "availability_bias": 0.0,
            "recency_bias": 0.0,
            "novelty_bias": 0.0
        }
        
        # M√©tricas de sistema
        self.system_metrics = {
            "total_assessments": 0,
            "approved_hypotheses": 0,
            "rejected_hypotheses": 0,
            "average_truth_score": 0.0,
            "temperature_adjustments": 0
        }
        
        logger.info("V10 - Agente Cr√≠tico S√™nior inicializado")

    async def assess_hypothesis(self, 
                              hypothesis: Dict[str, Any],
                              supporting_evidence: List[Dict[str, Any]],
                              contradicting_evidence: List[Dict[str, Any]] = None) -> CriticalAssessment:
        """
        Avalia uma hip√≥tese gerada pelo V9 usando Gemini e evid√™ncias do SFS.
        
        Args:
            hypothesis: Hip√≥tese do V9 AbductionEngine
            supporting_evidence: Evid√™ncias que suportam a hip√≥tese
            contradicting_evidence: Evid√™ncias que contradizem a hip√≥tese
            
        Returns:
            CriticalAssessment com pontua√ß√£o de risco e an√°lise detalhada
        """
        if contradicting_evidence is None:
            contradicting_evidence = []
            
        try:
            # 1. Preparar prompt para Gemini
            prompt = self._prepare_criticism_prompt(
                hypothesis, supporting_evidence, contradicting_evidence
            )
            
            # 2. Chamar Gemini para an√°lise cr√≠tica
            gemini_response = await self._call_gemini_critic(prompt)
            
            # 3. Analisar resposta do Gemini
            assessment = self._parse_gemini_response(
                hypothesis, gemini_response, supporting_evidence, contradicting_evidence
            )
            
            # 4. Atualizar hist√≥rico e m√©tricas
            self._update_assessment_history(assessment)
            self._update_system_metrics(assessment)
            
            # 5. Detectar padr√µes de bias
            self._detect_bias_patterns(assessment)
            
            logger.info(f"Hip√≥tese {hypothesis.get('id', 'unknown')} avaliada com score {assessment.truth_score:.3f}")
            return assessment
            
        except Exception as e:
            logger.error(f"Erro ao avaliar hip√≥tese: {str(e)}")
            # Retornar avalia√ß√£o conservadora em caso de erro
            return self._create_fallback_assessment(hypothesis)

    async def _call_gemini_critic(self, prompt: str) -> Dict[str, Any]:
        """
        Chama o Gemini para an√°lise cr√≠tica da hip√≥tese.
        
        Args:
            prompt: Prompt estruturado para o Gemini
            
        Returns:
            Resposta do Gemini em formato estruturado
        """
        if not self.gemini_api_key:
            logger.warning("Gemini API key n√£o configurada, usando fallback local")
            return self._simulate_gemini_response(prompt)
        
        try:
            # Implementa√ß√£o real da chamada para Gemini API
            url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"
            headers = {
                "Content-Type": "application/json"
            }
            
            payload = {
                "contents": [{
                    "parts": [{
                        "text": f"Evaluate this hypothesis with scientific rigor. Provide a truth score from 0 to 1, where 1 is definitely true and 0 is definitely false. Also provide reasoning. Hypothesis: {prompt}"
                    }]
                }],
                "generationConfig": {
                    "temperature": 0.1,
                    "maxOutputTokens": 500
                }
            }
            
            # Fazer chamada HTTP real para Gemini
            response = requests.post(
                f"{url}?key={self.gemini_api_key}",
                headers=headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                # Extrair resposta do Gemini
                text_response = data.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '')
                
                # Parser simples para extrair score
                score = self._parse_gemini_score(text_response)
                if score is None:
                    score = 0.5  # Fallback para score neutro
                
                return {
                    "truth_score": score,
                    "reasoning": text_response,
                    "raw_response": data
                }
            else:
                logger.error(f"Erro na API Gemini: {response.status_code} - {response.text}")
                return self._simulate_gemini_response(prompt)
                
        except Exception as e:
            logger.error(f"Erro na chamada ao Gemini: {str(e)}")
            return self._simulate_gemini_response(prompt)

    def _simulate_gemini_response(self, prompt: str) -> Dict[str, Any]:
        """
        Simula resposta do Gemini para demonstra√ß√£o com an√°lise cient√≠fica aprimorada.
        Em produ√ß√£o, substitua pela chamada real √† API.
        """
        # An√°lise cient√≠fica baseada em padr√µes
        score = 0.5  # Score neutro por padr√£o
        
        # Indicadores cient√≠ficos positivos
        positive_indicators = [
            "fundamental", "prerequisito", "base", "fundamenta√ß√£o", "essencial",
            "matem√°tica", "f√≠sica", "conceito", "teoria", "evid√™ncia",
            "confirmado", "validado", "estabelecido", "conhecido"
        ]
        
        negative_indicators = [
            "contradit√≥rio", "refutar", "incorreto", "falso", "errado",
            "inconsistente", "inv√°lido", "especulativo", "question√°vel"
        ]
        
        strong_evidence_indicators = [
            "evid√™ncia forte", "estudos mostram", "pesquisa demonstra",
            "reconhecido", "aceito", "estabelecido cientificamente"
        ]
        
        # Calcular score baseado em indicadores
        prompt_lower = prompt.lower()
        
        for indicator in positive_indicators:
            if indicator in prompt_lower:
                score += 0.15
        
        for indicator in negative_indicators:
            if indicator in prompt_lower:
                score -= 0.25
                
        for indicator in strong_evidence_indicators:
            if indicator in prompt_lower:
                score += 0.30
        
        # Bonus especial para conex√µes matem√°ticas-f√≠sicas conhecidas
        if "matem√°tica" in prompt_lower and "f√≠sica" in prompt_lower:
            score += 0.20
        if "√°lgebra" in prompt_lower and "qu√¢ntica" in prompt_lower:
            score += 0.25
        if "prerequisito" in prompt_lower or "fundamento" in prompt_lower:
            score += 0.20
            
        # Normalizar score
        score = max(0.0, min(1.0, score))
        
        # Determinar categoria e risco
        if score >= 0.8:
            category = "bem fundamentado"
            risk_level = "baixo"
            recommendation = "aprovar"
        elif score >= 0.6:
            category = "plaus√≠vel"
            risk_level = "medio"
            recommendation = "aprovar com cautela"
        elif score >= 0.4:
            category = "especulativo"
            risk_level = "medio"
            recommendation = "revisar"
        else:
            category = "question√°vel"
            risk_level = "alto"
            recommendation = "rejeitar"
        
        return {
            "truth_score": score,
            "risk_level": risk_level,
            "reasoning": f"An√°lise cient√≠fica baseada em padr√µes: {category} (score: {score:.2f})",
            "supporting_facts": self._extract_supporting_facts(prompt_lower),
            "contradicting_facts": self._extract_contradicting_facts(prompt_lower),
            "recommendation": recommendation,
            "coherence_score": min(1.0, score + 0.1)  # Coer√™ncia ligeiramente melhor que score
        }
    
    def _extract_supporting_facts(self, prompt: str) -> List[str]:
        """Extrai fatos de suporte do prompt"""
        facts = []
        if "matem√°tica" in prompt and "f√≠sica" in prompt:
            facts.append("Fundamentos matem√°ticos s√£o base para f√≠sica avan√ßada")
        if "√°lgebra" in prompt and "qu√¢ntica" in prompt:
            facts.append("Espa√ßos vetoriais s√£o essenciais na mec√¢nica qu√¢ntica")
        if "evid√™ncia" in prompt:
            facts.append("Evid√™ncias emp√≠ricas suportam a rela√ß√£o causal")
        if not facts:
            facts = ["Evid√™ncia conceitual 1", "Padr√£o cient√≠fico 2"]
        return facts
    
    def _extract_contradicting_facts(self, prompt: str) -> List[str]:
        """Extrai fatos contradit√≥rios do prompt"""
        facts = []
        if "contradit√≥rio" in prompt or "refutar" in prompt:
            facts.append("Evid√™ncia contr√°ria encontrada")
        if "question√°vel" in prompt:
            facts.append("Falta de evid√™ncia conclusiva")
        return facts

    def _parse_gemini_score(self, response_text: str) -> Optional[float]:
        """
        Extrai score num√©rico da resposta do Gemini
        """
        import re
        
        # Procurar padr√µes como "score: 0.7", "truth score: 0.7", etc.
        patterns = [
            r'score\s*[=:]\s*(\d*\.?\d+)',
            r'truth\s+score\s*[=:]\s*(\d*\.?\d+)',
            r'(\d*\.?\d+)\s*[/=]\s*1',
            r'(\d*\.?\d+)\s*por\s*cento',
            r'n√≠vel\s+de\s+confian√ßa\s*[=:]\s*(\d*\.?\d+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, response_text.lower())
            if match:
                try:
                    score = float(match.group(1))
                    # Normalizar se est√° em porcentagem
                    if score > 1:
                        score = score / 100
                    return max(0.0, min(1.0, score))  # Garantir que est√° entre 0 e 1
                except ValueError:
                    continue
        
        # Se n√£o conseguiu extrair, retornar None para usar fallback
        return None

    def _prepare_criticism_prompt(self, 
                                hypothesis: Dict[str, Any],
                                supporting_evidence: List[Dict],
                                contradicting_evidence: List[Dict]) -> str:
        """
        Prepara prompt estruturado para an√°lise cr√≠tica pelo Gemini.
        """
        prompt = f"""
        AN√ÅLISE CR√çTICA DE HIP√ìTESE - PROTOTYPE ALEXANDRIA
        
        HIP√ìTESE A AVALIAR:
        - ID: {hypothesis.get('id', 'N/A')}
        - Fonte: {hypothesis.get('source_cluster', 'N/A')}
        - Alvo: {hypothesis.get('target_cluster', 'N/A')}
        - Confian√ßa: {hypothesis.get('confidence_score', 'N/A')}
        - Descri√ß√£o: {hypothesis.get('description', 'N/A')}
        
        EVID√äNCIAS DE SUPORTE:
        {self._format_evidence_list(supporting_evidence)}
        
        EVID√äNCIAS CONTRADIT√ìRIAS:
        {self._format_evidence_list(contradicting_evidence)}
        
        INSTRU√á√ïES:
        1. Avalie a veracidade da hip√≥tese (0.0 a 1.0)
        2. Identifique riscos potenciais
        3. Liste fatos que suportam a hip√≥tese
        4. Liste fatos que contradizem a hip√≥tese
        5. Recomende: aprovar, revisar ou rejeitar
        
        Responda em formato JSON estruturado.
        """
        
        return prompt

    def _format_evidence_list(self, evidence_list: List[Dict]) -> str:
        """Formata lista de evid√™ncias para o prompt."""
        if not evidence_list:
            return "Nenhuma evid√™ncia dispon√≠vel."
            
        formatted = []
        for i, evidence in enumerate(evidence_list[:5], 1):  # Limitar a 5 evid√™ncias
            formatted.append(f"{i}. {evidence.get('content', 'N/A')} (Score: {evidence.get('similarity_score', 'N/A')})")
        
        return "\n".join(formatted)

    def _parse_gemini_response(self, 
                             hypothesis: Dict[str, Any],
                             gemini_response: Dict[str, Any],
                             supporting_evidence: List[Dict],
                             contradicting_evidence: List[Dict]) -> CriticalAssessment:
        """
        Analisa resposta do Gemini e cria CriticalAssessment.
        """
        truth_score = gemini_response.get("truth_score", 0.5)
        
        # Determinar categoria de veracidade
        if truth_score >= 0.8:
            truth_category = TruthScore.FACTUAL
        elif truth_score >= 0.6:
            truth_category = TruthScore.PLAUSIVEL
        elif truth_score >= 0.4:
            truth_category = TruthScore.ESPECULATIVO
        else:
            truth_category = TruthScore.FALSO
        
        # Determinar n√≠vel de risco
        if truth_score >= self.risk_tolerance:
            if truth_score >= 0.9:
                risk_level = RiskLevel.BAIXO
            else:
                risk_level = RiskLevel.MEDIO
        else:
            if truth_score >= 0.3:
                risk_level = RiskLevel.ALTO
            else:
                risk_level = RiskLevel.CRITICO
        
        # Calcular qualidade das evid√™ncias
        evidence_quality = self._calculate_evidence_quality(supporting_evidence, contradicting_evidence)
        
        # Criar intervalos de confian√ßa
        confidence_intervals = {
            "lower_bound": max(0.0, truth_score - 0.1),
            "upper_bound": min(1.0, truth_score + 0.1),
            "uncertainty": 0.1
        }
        
        # Determinar recomenda√ß√£o
        recommendation = gemini_response.get("recommendation")
        if not recommendation:
             # Fallback: derive from score
             if truth_score >= 0.8: recommendation = "aprovar"
             elif truth_score >= 0.6: recommendation = "aprovar com cautela"
             else: recommendation = "revisar"
        
        # Gerar ajustes sugeridos
        suggested_adjustments = self._generate_adjustment_suggestions(
            truth_score, evidence_quality, risk_level
        )
        
        return CriticalAssessment(
            hypothesis_id=hypothesis.get("id", "unknown"),
            truth_score=truth_score,
            truth_category=truth_category,
            risk_level=risk_level,
            confidence_intervals=confidence_intervals,
            evidence_quality=evidence_quality,
            contradiction_strength=len(contradicting_evidence) / max(len(supporting_evidence), 1),
            reasoning_coherence=gemini_response.get("coherence_score", 0.5),
            supporting_facts=gemini_response.get("supporting_facts", []),
            contradicting_facts=gemini_response.get("contradicting_facts", []),
            gaps_in_evidence=self._identify_evidence_gaps(supporting_evidence, contradicting_evidence),
            recommendation=recommendation,
            suggested_adjustments=suggested_adjustments,
            assessed_at=datetime.now()
        )

    def _calculate_evidence_quality(self, 
                                  supporting_evidence: List[Dict],
                                  contradicting_evidence: List[Dict]) -> float:
        """Calcula qualidade das evid√™ncias."""
        total_evidence = len(supporting_evidence) + len(contradicting_evidence)
        
        if total_evidence == 0:
            return 0.0
        
        # Calcular score m√©dio das evid√™ncias
        all_evidence = supporting_evidence + contradicting_evidence
        avg_score = np.mean([e.get('similarity_score', 0.5) for e in all_evidence])
        
        # Bonus por quantidade de evid√™ncias
        quantity_bonus = min(0.2, total_evidence * 0.02)
        
        return min(1.0, avg_score + quantity_bonus)

    def _identify_evidence_gaps(self, 
                              supporting_evidence: List[Dict],
                              contradicting_evidence: List[Dict]) -> List[str]:
        """Identifica lacunas nas evid√™ncias."""
        gaps = []
        
        if len(supporting_evidence) < 3:
            gaps.append("Evid√™ncias de suporte insuficientes")
        
        if len(contradicting_evidence) > len(supporting_evidence):
            gaps.append("Evid√™ncias contradit√≥rias superam as de suporte")
        
        if not supporting_evidence:
            gaps.append("Nenhuma evid√™ncia de suporte encontrada")
        
        return gaps

    def _generate_adjustment_suggestions(self, 
                                       truth_score: float,
                                       evidence_quality: float,
                                       risk_level: RiskLevel) -> List[str]:
        """Gera sugest√µes de ajuste baseadas na avalia√ß√£o."""
        suggestions = []
        
        if truth_score < 0.5:
            suggestions.append("Considerar gerar hip√≥teses mais conservadoras")
            suggestions.append("Revisar fontes de treinamento do V9")
        
        if evidence_quality < 0.6:
            suggestions.append("Expandir base de evid√™ncias no SFS")
            suggestions.append("Melhorar indexa√ß√£o sem√¢ntica")
        
        if risk_level == RiskLevel.CRITICO:
            suggestions.append("Implementar valida√ß√£o adicional obrigat√≥ria")
            suggestions.append("Reduzir temperatura criativa do TinyLlama")
        
        if risk_level == RiskLevel.ALTO:
            suggestions.append("Adicionar camada de revis√£o manual")
        
        return suggestions

    def _create_fallback_assessment(self, hypothesis: Dict[str, Any]) -> CriticalAssessment:
        """Cria avalia√ß√£o conservadora em caso de erro."""
        return CriticalAssessment(
            hypothesis_id=hypothesis.get("id", "unknown"),
            truth_score=0.3,  # Score conservador
            truth_category=TruthScore.ESPECULATIVO,
            risk_level=RiskLevel.ALTO,
            confidence_intervals={"lower_bound": 0.1, "upper_bound": 0.5, "uncertainty": 0.2},
            evidence_quality=0.0,
            contradiction_strength=1.0,
            reasoning_coherence=0.0,
            supporting_facts=[],
            contradicting_facts=[],
            gaps_in_evidence=["Falha na an√°lise cr√≠tica autom√°tica"],
            recommendation="revisar",
            suggested_adjustments=["Verificar configura√ß√£o do Gemini", "Investigar logs do sistema"],
            assessed_at=datetime.now()
        )

    def _update_assessment_history(self, assessment: CriticalAssessment):
        """Atualiza hist√≥rico de avalia√ß√µes."""
        self.assessment_history.append(assessment)
        
        # Manter apenas as √∫ltimas 1000 avalia√ß√µes para performance
        if len(self.assessment_history) > 1000:
            self.assessment_history = self.assessment_history[-1000:]

    def _update_system_metrics(self, assessment: CriticalAssessment):
        """Atualiza m√©tricas do sistema."""
        self.system_metrics["total_assessments"] += 1
        
        if assessment.recommendation == "aprovar":
            self.system_metrics["approved_hypotheses"] += 1
        elif assessment.recommendation == "rejeitar":
            self.system_metrics["rejected_hypotheses"] += 1
        
        # Atualizar m√©dia m√≥vel do truth score
        total = self.system_metrics["total_assessments"]
        current_avg = self.system_metrics["average_truth_score"]
        new_score = assessment.truth_score
        
        self.system_metrics["average_truth_score"] = (
            (current_avg * (total - 1) + new_score) / total
        )

    def _detect_bias_patterns(self, assessment: CriticalAssessment):
        """Detecta padr√µes de bias no sistema."""
        recent_assessments = self.assessment_history[-50:]  # √öltimas 50 avalia√ß√µes
        
        if not recent_assessments:
            return
        
        # Confirmation bias: alta taxa de aprova√ß√£o
        approval_rate = sum(1 for a in recent_assessments if a.recommendation == "aprovar") / len(recent_assessments)
        self.bias_indicators["confirmation_bias"] = max(0.0, approval_rate - 0.7)
        
        # Recency bias: scores mudando drasticamente
        recent_scores = [a.truth_score for a in recent_assessments[-10:]]
        if len(recent_scores) > 1:
            score_variance = np.var(recent_scores)
            self.bias_indicators["recency_bias"] = min(1.0, score_variance * 2)
        
        # Availability bias: excesso de evid√™ncias de um tipo
        total_support = sum(len(a.supporting_facts) for a in recent_assessments)
        total_contradict = sum(len(a.contradicting_facts) for a in recent_assessments)
        total_evidence = total_support + total_contradict
        
        if total_evidence > 0:
            support_ratio = total_support / total_evidence
            self.bias_indicators["availability_bias"] = abs(support_ratio - 0.5) * 2

    async def get_system_feedback(self) -> SystemFeedback:
        """
        Gera feedback do sistema para auto-regulagem.
        
        Returns:
            SystemFeedback com m√©tricas e recomenda√ß√µes
        """
        recent_assessments = self.assessment_history[-100:]  # √öltimas 100 avalia√ß√µes
        
        if not recent_assessments:
            return SystemFeedback(
                timestamp=datetime.now(),
                total_assessments=0,
                approval_rate=0.0,
                average_truth_score=0.0,
                common_contradictions=[],
                system_bias_indicators=self.bias_indicators,
                recommended_temperature_adjustment=0.0,
                recommended_variance_adjustment=0.0
            )
        
        # Calcular taxa de aprova√ß√£o
        approval_rate = sum(1 for a in recent_assessments if a.recommendation == "aprovar") / len(recent_assessments)
        
        # Calcular score m√©dio
        avg_truth_score = np.mean([a.truth_score for a in recent_assessments])
        
        # Identificar contradi√ß√µes comuns
        common_contradictions = self._find_common_contradictions(recent_assessments)
        
        # Calcular ajustes recomendados
        temp_adjustment, var_adjustment = self._calculate_parameter_adjustments(
            approval_rate, avg_truth_score, self.bias_indicators
        )
        
        return SystemFeedback(
            timestamp=datetime.now(),
            total_assessments=len(recent_assessments),
            approval_rate=approval_rate,
            average_truth_score=avg_truth_score,
            common_contradictions=common_contradictions,
            system_bias_indicators=self.bias_indicators.copy(),
            recommended_temperature_adjustment=temp_adjustment,
            recommended_variance_adjustment=var_adjustment
        )

    def _find_common_contradictions(self, assessments: List[CriticalAssessment]) -> List[str]:
        """Encontra contradi√ß√µes comuns nas avalia√ß√µes."""
        contradiction_count = {}
        
        for assessment in assessments:
            for contradiction in assessment.contradicting_facts:
                contradiction_count[contradiction] = contradiction_count.get(contradiction, 0) + 1
        
        # Retornar contradi√ß√µes mais comuns
        return [item[0] for item in sorted(contradiction_count.items(), key=lambda x: x[1], reverse=True)[:5]]

    def _calculate_parameter_adjustments(self, 
                                       approval_rate: float,
                                       avg_truth_score: float,
                                       bias_indicators: Dict[str, float]) -> Tuple[float, float]:
        """
        Calcula ajustes recomendados para par√¢metros do sistema.
        
        Returns:
            Tuple de (temperature_adjustment, variance_adjustment)
        """
        temp_adjustment = 0.0
        var_adjustment = 0.0
        
        # Ajustar temperatura baseada na taxa de aprova√ß√£o
        if approval_rate > 0.8:
            # Sistema muito permissivo, reduzir criatividade
            temp_adjustment = -0.1
        elif approval_rate < 0.4:
            # Sistema muito restritivo, permitir mais criatividade
            temp_adjustment = 0.1
        
        # Ajustar vari√¢ncia baseada em bias
        total_bias = sum(bias_indicators.values())
        if total_bias > 0.6:
            # Alto bias detectado, aumentar vari√¢ncia para diversidade
            var_adjustment = 0.05
        
        return temp_adjustment, var_adjustment

    def apply_system_adjustments(self, temperature_adjustment: float, variance_adjustment: float):
        """
        Aplica ajustes autom√°ticos aos par√¢metros do sistema.
        
        Args:
            temperature_adjustment: Ajuste de temperatura (-1.0 a 1.0)
            variance_adjustment: Ajuste de vari√¢ncia (-1.0 a 1.0)
        """
        # Registrar hist√≥rico de temperaturas
        current_temp = self.temperature_history[-1] if self.temperature_history else 0.7
        new_temp = max(0.0, min(1.0, current_temp + temperature_adjustment))
        self.temperature_history.append(new_temp)
        
        # Aplicar ajustes nos par√¢metros do TinyLlama (simulado)
        logger.info(f"Aplicando ajuste de temperatura: {temperature_adjustment:+.3f}")
        logger.info(f"Aplicando ajuste de vari√¢ncia: {variance_adjustment:+.3f}")
        
        self.system_metrics["temperature_adjustments"] += 1

    def export_assessment_report(self, format_type: str = "json") -> str:
        """
        Exporta relat√≥rio das avalia√ß√µes em formato espec√≠fico.
        
        Args:
            format_type: Tipo de formato ("json", "markdown")
            
        Returns:
            Relat√≥rio em formato especificado
        """
        if format_type == "json":
            report = {
                "metadata": {
                    "exported_at": datetime.now().isoformat(),
                    "total_assessments": len(self.assessment_history),
                    "system_metrics": self.system_metrics
                },
                "recent_assessments": [asdict(assessment) for assessment in self.assessment_history[-10:]],
                "bias_indicators": self.bias_indicators,
                "system_metrics": self.system_metrics  # Corrigido: era system_feedback
            }
            return json.dumps(report, indent=2, default=str)
        
        elif format_type == "markdown":
            return self._generate_markdown_report()
        
        else:
            raise ValueError(f"Formato n√£o suportado: {format_type}")

    def _generate_markdown_report(self) -> str:
        """Gera relat√≥rio em formato Markdown."""
        report = f"""# Relat√≥rio do Agente Cr√≠tico S√™nior - V10
        
## M√©tricas do Sistema
- **Total de Avalia√ß√µes**: {len(self.assessment_history)}
- **Taxa de Aprova√ß√£o**: {(self.system_metrics['approved_hypotheses'] / max(self.system_metrics['total_assessments'], 1) * 100):.1f}%
- **Score M√©dio de Veracidade**: {self.system_metrics['average_truth_score']:.3f}
- **Ajustes de Temperatura**: {self.system_metrics['temperature_adjustments']}

## Indicadores de Bias
"""
        
        for bias_type, score in self.bias_indicators.items():
            report += f"- **{bias_type.replace('_', ' ').title()}**: {score:.3f}\n"
        
        report += "\n## √öltimas 5 Avalia√ß√µes\n\n"
        
        for assessment in self.assessment_history[-5:]:
            report += f"### Hip√≥tese {assessment.hypothesis_id}\n"
            report += f"- **Score de Veracidade**: {assessment.truth_score:.3f}\n"
            report += f"- **N√≠vel de Risco**: {assessment.risk_level.value}\n"
            report += f"- **Recomenda√ß√£o**: {assessment.recommendation}\n"
            report += f"- **Avaliada em**: {assessment.assessed_at.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        return report