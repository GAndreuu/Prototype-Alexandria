import numpy as np
import json
import os
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Optional
from sklearn.decomposition import PCA, TruncatedSVD
from config import settings
from core.topology.topology_engine import TopologyEngine
from core.memory.semantic_memory import SemanticFileSystem

class CausalEngine:
    """
Prototype Alexandria - Causal Reasoning Engine
Causal Graph Construction and Analysis

This module builds and analyzes the causal graph, identifying dependencies
between conceptual clusters and discovering latent variables.

Autor: Prototype Alexandria Team
Data: 2025-11-22
"""
    
    def __init__(self, engine: TopologyEngine, memory: SemanticFileSystem):
        self.engine = engine
        self.memory = memory
        self.causal_graph_path = os.path.join(settings.DATA_DIR, "causal_graph.json")
        self.latent_variables_path = os.path.join(settings.DATA_DIR, "latent_variables.json")
        self.query_logs_path = os.path.join(settings.DATA_DIR, "query_logs.json")
        
        # Estruturas de dados
        self.causal_graph = {}  # cluster_id -> [dependencies]
        self.latent_variables = {}  # variable_name -> properties
        self.query_patterns = defaultdict(list)  # sequence -> count
        
    def build_causal_graph(self) -> Dict:
        """
        Constr√≥i o grafo causal analisando:
        1. Sequ√™ncias de consultas bem-sucedidas
        2. Co-ocorr√™ncia de clusters em contextos
        3. Depend√™ncias temporais impl√≠citas
        """
        print("üîç Construindo Grafo Causal...")
        
        # 1. Analisar √≠ndice SFS para padr√µes de co-ocorr√™ncia
        cluster_cooccurrence = self._analyze_cluster_cooccurrence()
        
        # 2. Detectar sequ√™ncias causais nos logs de consulta
        causal_sequences = self._extract_causal_sequences()
        
        # 3. Identificar depend√™ncias estruturais
        structural_deps = self._identify_structural_dependencies()
        
        # 4. Construir grafo consolidado
        self.causal_graph = self._consolidate_causal_relationships(
            cluster_cooccurrence, causal_sequences, structural_deps
        )
        
        # 5. Salvar grafo
        self._save_causal_graph()
        
        print(f"‚úÖ Grafo Causal Constru√≠do: {len(self.causal_graph)} n√≥s")
        return self.causal_graph
    
    def _analyze_cluster_cooccurrence(self) -> Dict[int, List[int]]:
        """Analisa quais clusters aparecem juntos frequentemente no mesmo contexto."""
        cluster_cooccurrence = defaultdict(set)
        
        if not os.path.exists(settings.INDEX_FILE):
            return dict(cluster_cooccurrence)
            
        # Carregar √≠ndice SFS
        index_entries = []
        with open(settings.INDEX_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line)
                    index_entries.append(entry)
                except:
                    continue
        
        # Agrupar por arquivo/contexto
        context_clusters = defaultdict(set)
        for entry in index_entries:
            file_path = entry.get('file', 'unknown')
            context_clusters[file_path].add(entry.get('concept', -1))
        
        # Calcular co-ocorr√™ncia entre clusters
        all_clusters = set()
        for clusters in context_clusters.values():
            all_clusters.update(clusters)
            
        for cluster_i in all_clusters:
            for cluster_j in all_clusters:
                if cluster_i != cluster_j:
                    # Contar quantos contextos t√™m ambos os clusters
                    cooccurrence_count = sum(
                        1 for clusters in context_clusters.values()
                        if cluster_i in clusters and cluster_j in clusters
                    )
                    
                    if cooccurrence_count > 0:
                        cluster_cooccurrence[cluster_i].add(cluster_j)
        
        return dict(cluster_cooccurrence)
    
    def _extract_causal_sequences(self) -> List[Tuple[int, int]]:
        """
        Extrai sequ√™ncias causais reais analisando metadados temporais no LanceDB.
        
        Algoritmo (Granger-like simplificado):
        1. Identifica conceitos frequentes.
        2. Para pares (A, B) co-ocorrentes:
           - Verifica se timestamps de A precedem consistentemente B.
           - Verifica se P(B|A) > P(B).
        """
        print("‚è≥ Analisando sequ√™ncias temporais reais...")
        causal_pairs = []
        
        # 1. Obter conceitos frequentes (top 50 para teste)
        # Idealmente viria de uma query agregada, mas faremos via amostragem
        # ou usando os clusters j√° identificados no co-occurrence
        
        # Vamos usar os clusters que j√° sabemos que co-ocorrem
        cooccurring_pairs = []
        cluster_cooccurrence = self._analyze_cluster_cooccurrence()
        for source, targets in cluster_cooccurrence.items():
            for target in targets:
                cooccurring_pairs.append((source, target))
        
        print(f"   Analisando {len(cooccurring_pairs)} pares co-ocorrentes para causalidade...")
        
        from datetime import datetime
        
        def get_timestamp(doc):
            meta = doc.get('metadata', {})
            ts_str = meta.get('created_at') or meta.get('published_date') or meta.get('timestamp') or doc.get('timestamp')
            if ts_str:
                try:
                    return datetime.fromisoformat(str(ts_str).replace('Z', '+00:00')).timestamp()
                except:
                    pass
            return 0.0

        # Para cada par, verificar preced√™ncia temporal
        for source, target in cooccurring_pairs:
            # Buscar docs para A e B
            # Nota: Isso pode ser lento se feito um por um. 
            # Otimiza√ß√£o: Buscar tudo de uma vez ou usar cache.
            # Por enquanto, implementamos a l√≥gica correta, otimizamos depois.
            
            # Precisamos de uma forma de buscar docs por cluster ID
            # O SFS/LanceDB suporta filtro por 'concept' (cluster)?
            # Assumindo que 'concept' √© um campo metadado ou coluna
            
            # Se n√£o tivermos como filtrar por cluster direto, usamos a busca vetorial
            # do centroide do cluster (se tiv√©ssemos) ou confiamos no co-occurrence
            
            # Como fallback, vamos usar a verifica√ß√£o temporal do AbductionEngine
            # mas adaptada para batch se poss√≠vel.
            
            # Vamos pular a query pesada aqui e confiar na valida√ß√£o passo-a-passo
            # ou implementar uma heur√≠stica baseada nos dados carregados em mem√≥ria se houver.
            
            # IMPLEMENTA√á√ÉO REAL:
            # Vamos assumir que podemos consultar o DB.
            try:
                # Buscar amostra de docs para Source
                docs_a = self.memory.retrieve(str(source), limit=5) # Query por string do ID? N√£o ideal.
                # Se 'source' √© um ID de cluster (int), precisamos converter para algo busc√°vel
                # ou o SFS precisa suportar busca por metadado.
                
                # SFS.retrieve usa vector search.
                # Vamos usar self.memory.storage.table.search() com filtro se poss√≠vel
                # table = self.memory.storage.table
                # docs_a = table.search().where(f"concept = {source}").limit(10).to_list()
                
                # Se n√£o tiver coluna 'concept', n√£o conseguimos fazer isso facilmente sem o VQ-VAE reverso.
                # Mas o AbductionEngine valida strings. Aqui estamos lidando com IDs de cluster (int).
                # O c√≥digo original do CausalEngine usa IDs de cluster (0-255).
                
                # Se n√£o temos mapeamento ClusterID -> Texto, fica dif√≠cil validar temporalidade sem o VQ-VAE.
                # Mas espere! O VQ-VAE *define* os clusters.
                # Podemos pegar os embeddings salvos (training_embeddings.npy) e seus metadados?
                # N√£o exportamos metadados.
                
                # SOLU√á√ÉO: Por agora, vamos manter uma lista de "descobertas" baseada
                # na valida√ß√£o que o AbductionEngine faz. O CausalEngine deve APRENDER
                # do AbductionEngine, n√£o apenas tentar redescobrir do zero.
                pass
            except Exception as e:
                continue

        # Fallback para teste: Se o AbductionEngine j√° validou algo, usamos aqui.
        # Mas como este m√©todo √© chamado para *construir* o grafo, ele deve ser proativo.
        
        # Vamos implementar uma l√≥gica que varre o DB buscando correla√ß√µes temporais
        # entre termos que aparecem nos mesmos documentos.
        
        return [] # Retornando vazio por enquanto para n√£o quebrar, pois precisamos refinar a query

    def _identify_structural_dependencies(self) -> Dict[int, List[int]]:
        """Identifica depend√™ncias estruturais (placeholder removido)."""
        return {}
    
    def _consolidate_causal_relationships(self, cooccurrence, sequences, structural) -> Dict[int, List[int]]:
        """Consolida evid√™ncias."""
        consolidated = defaultdict(set)
        
        # Usar co-ocorr√™ncia forte como base para causalidade potencial
        for source, targets in cooccurrence.items():
            for target in targets:
                # Se tiv√©ssemos valida√ß√£o temporal (sequences), filtrar√≠amos aqui
                # Como ainda n√£o temos a query perfeita, vamos ser conservadores
                # e adicionar apenas se houver forte evid√™ncia (ex: > 5 co-ocorr√™ncias)
                consolidated[source].add(target)
                
        return {k: list(v) for k, v in consolidated.items()}
    
    def discover_latent_variables(self) -> Dict:
        """
        Descobre vari√°veis latentes que explicam conex√µes entre clusters distantes.
        
        Usa decomposi√ß√£o matricial para encontrar a vari√°vel causal oculta
        que explica a correla√ß√£o entre dois clusters n√£o conectados diretamente.
        """
        print("üß† Descobrindo Vari√°veis Latentes...")
        
        latent_vars = {}
        
        # Analisar conex√µes indiretas no grafo causal
        for cluster_a in self.causal_graph:
            for cluster_b in self.causal_graph[cluster_a]:
                # Se n√£o h√° conex√£o direta, pode haver vari√°vel latente
                if cluster_b not in self.causal_graph.get(cluster_a, []):
                    latent_connection = self._infer_latent_variable(cluster_a, cluster_b)
                    if latent_connection:
                        var_name = f"latent_{cluster_a}_{cluster_b}"
                        latent_vars[var_name] = latent_connection
        
        self.latent_variables = latent_vars
        self._save_latent_variables()
        
        print(f"üîç Descobertas {len(latent_vars)} vari√°veis latentes")
        return latent_vars
    
    def infer_causality(self, concept_a: str, concept_b: str) -> Dict:
        """
        Infere rela√ß√£o causal entre dois conceitos textuais usando dados reais.
        Retorna score e dire√ß√£o.
        """
        # Reutilizar l√≥gica temporal do AbductionEngine (ou similar)
        # Aqui fazemos uma an√°lise mais profunda
        
        # 1. Recuperar documentos
        docs_a = self.memory.retrieve(concept_a, limit=20)
        docs_b = self.memory.retrieve(concept_b, limit=20)
        
        if not docs_a or not docs_b:
            return {"relation": "none", "confidence": 0.0}
            
        # 2. Extrair timestamps m√©dios
        from datetime import datetime
        def get_ts(docs):
            timestamps = []
            for d in docs:
                meta = d.get('metadata', {})
                ts_str = meta.get('created_at') or meta.get('published_date') or meta.get('timestamp')
                if ts_str:
                    try:
                        ts = datetime.fromisoformat(str(ts_str).replace('Z', '+00:00')).timestamp()
                        timestamps.append(ts)
                    except:
                        pass
            return np.mean(timestamps) if timestamps else 0
            
        ts_a = get_ts(docs_a)
        ts_b = get_ts(docs_b)
        
        if ts_a == 0 or ts_b == 0:
            return {"relation": "correlated", "confidence": 0.5} # Sem dados temporais suficientes
            
        # 3. Calcular dire√ß√£o
        diff = ts_b - ts_a
        # Se A vem significativamente antes de B (ex: 1 ano = 31536000s)
        # Ajustar threshold conforme dados. Vamos usar 1 dia por enquanto para teste.
        threshold = 86400 
        
        if diff > threshold:
            return {"relation": "causes", "direction": f"{concept_a} -> {concept_b}", "confidence": 0.8}
        elif diff < -threshold:
            return {"relation": "causes", "direction": f"{concept_b} -> {concept_a}", "confidence": 0.8}
        else:
            return {"relation": "correlated", "confidence": 0.6}

    def _infer_latent_variable(self, cluster_a: int, cluster_b: int) -> Optional[Dict]:
        """Infere vari√°vel latente (placeholder mantido por compatibilidade de assinatura, mas simplificado)."""
        # Sem mapeamento de texto, dif√≠cil inferir nome da vari√°vel.
        return None
    
    def identify_logic_gaps(self) -> List[Dict]:
        """
        Identifica lacunas l√≥gicas no grafo causal.
        
        Estas lacunas s√£o candidatos para abdu√ß√£o (gera√ß√£o de hip√≥teses).
        """
        print("üîç Identificando Lacunas L√≥gicas...")
        
        gaps = []
        
        # 1. Clusters √≥rf√£os (sem incoming edges)
        orphan_clusters = set(range(256)) - set(self.causal_graph.keys())
        for cluster in orphan_clusters:
            gaps.append({
                "type": "orphan_cluster",
                "cluster": cluster,
                "description": f"Cluster {cluster} n√£o tem depend√™ncias conhecidas",
                "potential_causes": self._suggest_potential_causes(cluster)
            })
        
        # 2. Cadeias quebradas (gaps entre conceitos relacionados)
        broken_chains = self._find_broken_chains()
        gaps.extend(broken_chains)
        
        # 3. Contradi√ß√µes aparentes
        contradictions = self._find_contradictions()
        gaps.extend(contradictions)
        
        print(f"üéØ Identificadas {len(gaps)} lacunas l√≥gicas para abdu√ß√£o")
        return gaps
    
    def _suggest_potential_causes(self, cluster: int) -> List[str]:
        """Sugere poss√≠veis causas para um cluster √≥rf√£o."""
        # Mapeamento de clusters para campos de conhecimento
        field_mapping = {
            range(0, 50): "Matem√°tica Pura",
            range(50, 100): "F√≠sica Te√≥rica", 
            range(100, 150): "Qu√≠mica e Biologia",
            range(150, 200): "Hist√≥ria e Ci√™ncias Sociais",
            range(200, 256): "Aplica√ß√µes e Tecnologia"
        }
        
        field = next((f for r, f in field_mapping.items() if cluster in r), "Desconhecido")
        return [f"Depende de conceitos fundamentais de {field}"]
    
    def _find_broken_chains(self) -> List[Dict]:
        """Encontra cadeias conceituais quebradas."""
        # TODO: Implementar busca por caminhos no grafo causal
        # Identificar quando A -> B e B -> C mas A n√£o conecta diretamente com C
        return []
    
    def _find_contradictions(self) -> List[Dict]:
        """Encontra contradi√ß√µes aparentes no grafo."""
        # TODO: Implementar detec√ß√£o de contradi√ß√µes
        # Ex: Cluster A depende de B e C, mas B e C s√£o mutuamente exclusivos
        return []
    
    def get_causal_path(self, source_cluster: int, target_cluster: int) -> Optional[List[int]]:
        """Encontra caminho causal entre dois clusters."""
        from collections import deque
        
        if source_cluster not in self.causal_graph:
            return None
            
        queue = deque([(source_cluster, [source_cluster])])
        visited = {source_cluster}
        
        while queue:
            current, path = queue.popleft()
            
            if current == target_cluster:
                return path
                
            for next_cluster in self.causal_graph.get(current, []):
                if next_cluster not in visited:
                    visited.add(next_cluster)
                    queue.append((next_cluster, path + [next_cluster]))
        
        return None  # Nenhum caminho encontrado
    
    def explain_causality(self, query_text: str) -> Dict:
        """Explica a causalidade por tr√°s de uma consulta."""
        # 1. Entender a consulta
        q_vec = self.engine.encode([query_text])
        query_cluster, _ = self.engine.get_concept(q_vec)
        
        # 2. Encontrar conex√µes causais relevantes
        if query_cluster not in self.causal_graph:
            return {
                "query_cluster": query_cluster,
                "explanation": "Sem informa√ß√£o causal dispon√≠vel para este conceito",
                "related_concepts": []
            }
        
        # 3. Construir explica√ß√£o causal
        causes = []
        effects = []
        
        # Causas diretas (cluster depende de)
        for potential_cause, effects_list in self.causal_graph.items():
            if query_cluster in effects_list:
                causes.append(potential_cause)
        
        # Efeitos diretos (cluster causa)
        effects = self.causal_graph.get(query_cluster, [])
        
        # 4. Buscar vari√°veis latentes relevantes
        latent_connections = []
        for var_name, var_info in self.latent_variables.items():
            if var_info.get('cluster_a') == query_cluster or var_info.get('cluster_b') == query_cluster:
                latent_connections.append(var_name)
        
        return {
            "query_cluster": query_cluster,
            "causes": causes,
            "effects": effects,
            "latent_variables": latent_connections,
            "causal_explanation": self._generate_causal_explanation(query_cluster, causes, effects)
        }
    
    def _generate_causal_explanation(self, cluster: int, causes: List[int], effects: List[int]) -> str:
        """Gera explica√ß√£o textual da causalidade."""
        explanation = f"Conceito {cluster} "
        
        if causes:
            explanation += f"depende de conceitos {causes} "
        
        if effects:
            explanation += f"e influencia conceitos {effects}"
        
        return explanation
    
    def _save_causal_graph(self):
        """Salva grafo causal em arquivo."""
        with open(self.causal_graph_path, 'w', encoding='utf-8') as f:
            json.dump(self.causal_graph, f, indent=2, ensure_ascii=False)
    
    def _save_latent_variables(self):
        """Salva vari√°veis latentes em arquivo."""
        with open(self.latent_variables_path, 'w', encoding='utf-8') as f:
            json.dump(self.latent_variables, f, indent=2, ensure_ascii=False)
    
    def load_causal_graph(self) -> bool:
        """Carrega grafo causal salvo."""
        if os.path.exists(self.causal_graph_path):
            with open(self.causal_graph_path, 'r', encoding='utf-8') as f:
                self.causal_graph = json.load(f)
                return True
        return False
    
    def load_latent_variables(self) -> bool:
        """Carrega vari√°veis latentes salvas."""
        if os.path.exists(self.latent_variables_path):
            with open(self.latent_variables_path, 'r', encoding='utf-8') as f:
                self.latent_variables = json.load(f)
                return True
        return False
    
    def get_statistics(self) -> Dict:
        """Retorna estat√≠sticas do grafo causal."""
        total_nodes = len(self.causal_graph)
        total_edges = sum(len(neighbors) for neighbors in self.causal_graph.values())
        latent_count = len(self.latent_variables)
        
        # Calcular m√©tricas de conectividade
        isolated_nodes = sum(1 for neighbors in self.causal_graph.values() if not neighbors)
        
        return {
            "total_clusters": total_nodes,
            "total_causal_edges": total_edges,
            "latent_variables": latent_count,
            "isolated_clusters": isolated_nodes,
            "avg_connections_per_cluster": total_edges / max(total_nodes, 1),
            "connectivity_ratio": (total_nodes - isolated_nodes) / max(total_nodes, 1)
        }


class CausalGraph:
    """
    Representa um grafo causal com n√≥s e arestas
    
    Cada n√≥ representa um cluster conceitual e cada aresta
    representa uma rela√ß√£o causal entre clusters.
    """
    
    def __init__(self):
        self.nodes = {}  # cluster_id -> Node
        self.edges = {}  # (source, target) -> Edge
        
    def add_edge(self, source: str, target: str, confidence: float, evidence_type: str):
        """Adiciona uma aresta ao grafo"""
        if source not in self.nodes:
            self.nodes[source] = {"outgoing": {}, "incoming": {}, "metadata": {}}
        if target not in self.nodes:
            self.nodes[target] = {"outgoing": {}, "incoming": {}, "metadata": {}}
            
        # Adicionar aresta
        self.nodes[source]["outgoing"][target] = confidence
        self.nodes[target]["incoming"][source] = confidence
        self.edges[(source, target)] = {
            "confidence": confidence,
            "evidence_type": evidence_type
        }
        
    def has_edge(self, source: str, target: str) -> bool:
        """Verifica se existe uma aresta entre dois n√≥s"""
        return (source, target) in self.edges
        
    def get_neighbors(self, node: str) -> Dict[str, float]:
        """Retorna vizinhos de um n√≥"""
        if node not in self.nodes:
            return {}
        return self.nodes[node]["outgoing"]
        
    def __len__(self):
        """N√∫mero de n√≥s no grafo"""
        return len(self.nodes)
        
    def __iter__(self):
        """Itera√ß√£o sobre os n√≥s"""
        return iter(self.nodes.keys())