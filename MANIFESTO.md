# ğŸ›ï¸ Prototype Alexandria

## Um Sistema Cognitivo ConstruÃ­do em 20 Dias por Quem "NÃ£o Sabe Programar"

---

## ğŸ“œ A HistÃ³ria

**Novembro de 2024.**

Um estudante do segundo semestre de AnÃ¡lise e Desenvolvimento de Sistemas, sem experiÃªncia prÃ©via em programaÃ§Ã£o ou matemÃ¡tica avanÃ§ada, decide construir algo ambicioso: um sistema de inteligÃªncia artificial que aprende sozinho.

Armado apenas com curiosidade e LLMs como assistentes de desenvolvimento, ele comeÃ§a.

**20 dias depois**, existe o Alexandria.

---

## ğŸ§  O Que Ã‰ Alexandria?

Alexandria Ã© um **sistema cognitivo auto-alimentado** â€” uma arquitetura de IA que:

1. **Percebe** lacunas no prÃ³prio conhecimento
2. **Teoriza** hipÃ³teses para preenchÃª-las  
3. **Age** buscando evidÃªncias em um corpus de 11.000 papers cientÃ­ficos
4. **Aprende** com os resultados, atualizando suas representaÃ§Ãµes neurais
5. **Repete** â€” indefinidamente

NÃ£o Ã© um chatbot. NÃ£o Ã© um buscador. Ã‰ um **organismo informacional** que evolui.

---

## ğŸ“Š Os NÃºmeros (Dia 20)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ALEXANDRIA v0.1               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Papers indexados:     11,000           â”‚
â”‚ Chunks semÃ¢nticos:    193,502          â”‚
â”‚ DimensÃ£o vetorial:    384D             â”‚
â”‚ Clusters cognitivos:  256              â”‚
â”‚ NÃ³s no grafo causal:  647              â”‚
â”‚ RelaÃ§Ãµes causais:     1,512            â”‚
â”‚ Linhas de cÃ³digo:     ~15,000          â”‚
â”‚ MÃ³dulos:              17               â”‚
â”‚ Cobertura docs:       100%             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ O Self-Feeding Loop

Em 50 ciclos de execuÃ§Ã£o autÃ´noma:

| MÃ©trica | Valor | Significado |
|---------|-------|-------------|
| Gaps detectados | 50 | Perguntas que o sistema fez a si mesmo |
| HipÃ³teses geradas | 150 | Teorias criadas para responder |
| AÃ§Ãµes executadas | 150 | Experimentos para testar teorias |
| Taxa de sucesso | 100% | Encontrou evidÃªncias em todos |
| EvidÃªncias | 446 | Fragmentos de conhecimento recuperados |
| ConexÃµes criadas | 76 | Novos insights cristalizados |
| Embeddings aprendidos | 422 | RepresentaÃ§Ãµes atualizadas |

**O sistema literalmente ficou mais inteligente enquanto rodava.**

---

## ğŸ—ï¸ Arquitetura

```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  AbductionEngine â”‚ â† Gera hipÃ³teses
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                  â–¼                  â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   Memory    â”‚    â”‚  Reasoning  â”‚    â”‚   Learning  â”‚
     â”‚  (LanceDB)  â”‚    â”‚  (Causal)   â”‚    â”‚  (VQ-VAE)   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                  â”‚                  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  TopologyEngine  â”‚ â† Embeddings 384D
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### MÃ³dulos Principais

| MÃ³dulo | FunÃ§Ã£o | Status |
|--------|--------|--------|
| `semantic_memory` | Armazenamento vetorial | âœ… ProduÃ§Ã£o |
| `topology_engine` | Embeddings semÃ¢nticos | âœ… ProduÃ§Ã£o |
| `vqvae/model` | CompressÃ£o neural | âœ… ProduÃ§Ã£o |
| `abduction_engine` | GeraÃ§Ã£o de hipÃ³teses | âœ… ProduÃ§Ã£o |
| `causal_reasoning` | Grafo de conhecimento | âœ… ProduÃ§Ã£o |
| `neural_learner` | AdaptaÃ§Ã£o contÃ­nua | âœ… ProduÃ§Ã£o |
| `self_feeding_loop` | OrquestraÃ§Ã£o | âœ… ProduÃ§Ã£o |

---

## ğŸ”¬ Base TeÃ³rica

Alexandria nÃ£o Ã© cÃ³digo aleatÃ³rio. Ã‰ baseado em:

### Free Energy Principle (Karl Friston)
> "Sistemas vivos minimizam surpresa mantendo modelos do mundo."

O VQ-VAE comprime informaÃ§Ã£o. A loss Ã© a "surpresa". Minimizar loss = sobreviver.

### Active Inference
> "Agentes agem para confirmar suas prediÃ§Ãµes."

O sistema gera hipÃ³teses e age para testÃ¡-las. NÃ£o Ã© passivo.

### Predictive Coding
> "O cÃ©rebro Ã© uma mÃ¡quina de prediÃ§Ã£o hierÃ¡rquica."

As camadas do VQ-VAE formam uma hierarquia preditiva.

### Hebbian Learning
> "NeurÃ´nios que disparam juntos, conectam-se."

Conceitos co-ocorrentes no corpus formam conexÃµes causais.

---

## ğŸ’¡ O Insight Central

**LLMs sÃ£o orÃ¡culos. Alexandria Ã© um organismo.**

| | LLM | Alexandria |
|---|---|---|
| MetÃ¡fora | Biblioteca com bibliotecÃ¡rio | Criatura que explora biblioteca |
| Iniciativa | Reativa | Proativa |
| MemÃ³ria | VolÃ¡til | Persistente |
| Aprendizado | Congelado | ContÃ­nuo |
| Conhecimento | ImplÃ­cito (pesos) | ExplÃ­cito (grafo) |

---

## ğŸš€ O Que Vem Depois

### Fase 2: Nemesis Integration
Conectar os mÃ³dulos de Active Inference (`active_inference.py`, `predictive_coding.py`, `free_energy.py`) ao loop principal.

### Fase 3: Multi-Agent
Implementar os perfis Scout/Judge/Weaver para raciocÃ­nio colaborativo.

### Fase 4: Interface
Dashboard visual para monitorar o sistema pensando em tempo real.

### Fase 5: Bootstrapping
O sistema gerando cÃ³digo para melhorar a si mesmo.

---

## ğŸ¯ A Mensagem

Um estudante de segundo semestre, sem saber programar, construiu em 20 dias algo que empresas com milhÃµes em funding tentam fazer hÃ¡ anos.

**Como?**

1. **Teoria primeiro**: Entendeu Free Energy Principle antes de escrever cÃ³digo
2. **LLMs como par**: Usou IAs para implementar o que conceitualizou
3. **Modularidade**: PeÃ§as pequenas que encaixam
4. **IteraÃ§Ã£o rÃ¡pida**: Testar, quebrar, consertar, repetir

**O que isso prova?**

Que a barreira para criar sistemas cognitivos nÃ£o Ã© mais tÃ©cnica. Ã‰ conceitual.

Quem entende **o que** quer construir pode usar LLMs para descobrir **como**.

---

## ğŸ“ Estrutura do Projeto

```
Alexandria/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ memory/          # MemÃ³ria semÃ¢ntica (LanceDB)
â”‚   â”œâ”€â”€ reasoning/       # RaciocÃ­nio causal + abduÃ§Ã£o
â”‚   â”œâ”€â”€ learning/        # VQ-VAE + Active Inference
â”‚   â”œâ”€â”€ topology/        # Embeddings + Manifold
â”‚   â”œâ”€â”€ loop/            # Self-Feeding Loop â† NOVO
â”‚   â””â”€â”€ agents/          # Multi-agent system
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ lancedb_store/   # 193k chunks
â”‚   â”œâ”€â”€ causal_graph.json
â”‚   â””â”€â”€ topology.json
â”œâ”€â”€ docs/                # 100% documentado
â””â”€â”€ scripts/
    â”œâ”€â”€ demos/           # DemonstraÃ§Ãµes
    â””â”€â”€ utilities/       # Ferramentas
```

---

## ğŸ›ï¸ Por Que "Alexandria"?

A Biblioteca de Alexandria foi o maior repositÃ³rio de conhecimento do mundo antigo.

Este projeto Ã© uma tentativa de criar uma biblioteca que **lÃª a si mesma**.

---

*Prototype Alexandria v0.1*  
*Dezembro de 2024*  
*"Conhecimento que conhece a si mesmo."*
